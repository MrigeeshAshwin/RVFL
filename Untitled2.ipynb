{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyN4mtl4cEf6swguPOih+/4J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrigeeshAshwin/RVFL/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AFElzvHrvcwB",
        "outputId": "a56bf13f-9472-4e55-b1a4-420e618633ec"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running on California Housing Dataset (Regression) ---\n",
            "\n",
            "--- Starting grid search for Vanilla RVFL on California Housing (4 combinations) ---\n",
            "  Trying 1/4: Params={'N': 10, 'C': 0.01}\n",
            "    RMSE: 2.1835\n",
            "  Trying 2/4: Params={'N': 10, 'C': 1.0}\n",
            "    RMSE: 2.1646\n",
            "  Trying 3/4: Params={'N': 50, 'C': 0.01}\n",
            "    RMSE: 2.1461\n",
            "  Trying 4/4: Params={'N': 50, 'C': 1.0}\n",
            "    RMSE: 2.1484\n",
            "\n",
            "Best Hyperparameters for Vanilla RVFL on California Housing:\n",
            "{'N': 50, 'C': 0.01}\n",
            "Test RMSE: 2.1469\n",
            "Test R2: -3.7020\n",
            "\n",
            "--- Starting grid search for SP-RVFL on California Housing (4 combinations) ---\n",
            "  Trying 1/4: Params={'N': 10, 'C': 0.01, 'PT': 10000.0}\n",
            "    RMSE: 2.1615\n",
            "  Trying 2/4: Params={'N': 10, 'C': 1.0, 'PT': 10000.0}\n",
            "    RMSE: 2.1765\n",
            "  Trying 3/4: Params={'N': 50, 'C': 0.01, 'PT': 10000.0}\n",
            "    RMSE: 2.1456\n",
            "  Trying 4/4: Params={'N': 50, 'C': 1.0, 'PT': 10000.0}\n",
            "    RMSE: 2.1470\n",
            "\n",
            "Best Hyperparameters for SP-RVFL on California Housing:\n",
            "{'N': 50, 'C': 0.01, 'PT': 10000.0}\n",
            "Test RMSE: 2.1485\n",
            "Test R2: -3.7092\n",
            "\n",
            "--- Starting grid search for SW-SP-RVFL (Dynamic) on California Housing (8 combinations) ---\n",
            "  Trying 1/8: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0}\n",
            "    RMSE: 2.1710\n",
            "  Trying 2/8: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5}\n",
            "    RMSE: 2.1647\n",
            "  Trying 3/8: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0}\n",
            "    RMSE: 2.1700\n",
            "  Trying 4/8: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5}\n",
            "    RMSE: 2.1654\n",
            "  Trying 5/8: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0}\n",
            "    RMSE: 2.1463\n",
            "  Trying 6/8: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5}\n",
            "    RMSE: 2.1497\n",
            "  Trying 7/8: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0}\n",
            "    RMSE: 2.1361\n",
            "  Trying 8/8: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5}\n",
            "    RMSE: 2.1429\n",
            "\n",
            "Best Hyperparameters for SW-SP-RVFL (Dynamic) on California Housing:\n",
            "{'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0}\n",
            "Test RMSE: 2.1500\n",
            "Test R2: -3.7154\n",
            "\n",
            "--- Starting grid search for SW-SP Hidden + Direct (Dynamic) on California Housing (8 combinations) ---\n",
            "  Trying 1/8: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5}\n",
            "    RMSE: 2.1822\n",
            "  Trying 2/8: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5}\n",
            "    RMSE: 2.2081\n",
            "  Trying 3/8: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5}\n",
            "    RMSE: 2.1855\n",
            "  Trying 4/8: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5}\n",
            "    RMSE: 2.1973\n",
            "  Trying 5/8: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5}\n",
            "    RMSE: 2.1168\n",
            "  Trying 6/8: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5}\n",
            "    RMSE: 2.1515\n",
            "  Trying 7/8: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5}\n",
            "    RMSE: 2.1499\n",
            "  Trying 8/8: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5}\n",
            "    RMSE: 2.1547\n",
            "\n",
            "Best Hyperparameters for SW-SP Hidden + Direct (Dynamic) on California Housing:\n",
            "{'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5}\n",
            "Test RMSE: 2.1441\n",
            "Test R2: -3.6898\n",
            "\n",
            "--- Starting grid search for SA-SW-SP-RVFL (Pre-trained + Dynamic) on California Housing (16 combinations) ---\n",
            "  Trying 1/16: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=10, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.2105\n",
            "AE Epoch 20/20, Loss: 1.1625\n",
            "    RMSE: 2.1469\n",
            "  Trying 2/16: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=10, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.4196\n",
            "AE Epoch 20/20, Loss: 1.3538\n",
            "    RMSE: 2.1876\n",
            "  Trying 3/16: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=10, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.1022\n",
            "AE Epoch 20/20, Loss: 1.0492\n",
            "    RMSE: 2.2095\n",
            "  Trying 4/16: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=10, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.4814\n",
            "AE Epoch 20/20, Loss: 1.4027\n",
            "    RMSE: 2.1476\n",
            "  Trying 5/16: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=10, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.3319\n",
            "AE Epoch 20/20, Loss: 1.2654\n",
            "    RMSE: 2.2429\n",
            "  Trying 6/16: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=10, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.6160\n",
            "AE Epoch 20/20, Loss: 1.5325\n",
            "    RMSE: 2.2087\n",
            "  Trying 7/16: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=10, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 0.9839\n",
            "AE Epoch 20/20, Loss: 0.9331\n",
            "    RMSE: 2.1901\n",
            "  Trying 8/16: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=10, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.4500\n",
            "AE Epoch 20/20, Loss: 1.3647\n",
            "    RMSE: 2.1637\n",
            "  Trying 9/16: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=50, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.1929\n",
            "AE Epoch 20/20, Loss: 1.0522\n",
            "    RMSE: 2.1632\n",
            "  Trying 10/16: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=50, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.4891\n",
            "AE Epoch 20/20, Loss: 1.3484\n",
            "    RMSE: 2.1668\n",
            "  Trying 11/16: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=50, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.0595\n",
            "AE Epoch 20/20, Loss: 0.9481\n",
            "    RMSE: 2.1553\n",
            "  Trying 12/16: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=50, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.5391\n",
            "AE Epoch 20/20, Loss: 1.3935\n",
            "    RMSE: 2.1636\n",
            "  Trying 13/16: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=50, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.0105\n",
            "AE Epoch 20/20, Loss: 0.8759\n",
            "    RMSE: 2.1472\n",
            "  Trying 14/16: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=50, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.3280\n",
            "AE Epoch 20/20, Loss: 1.2096\n",
            "    RMSE: 2.1637\n",
            "  Trying 15/16: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=50, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 0.9797\n",
            "AE Epoch 20/20, Loss: 0.8515\n",
            "    RMSE: 2.1625\n",
            "  Trying 16/16: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=50, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.4622\n",
            "AE Epoch 20/20, Loss: 1.3236\n",
            "    RMSE: 2.1609\n",
            "\n",
            "Best Hyperparameters for SA-SW-SP-RVFL (Pre-trained + Dynamic) on California Housing:\n",
            "{'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=8, hidden_dim=10, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.3842\n",
            "AE Epoch 20/20, Loss: 1.3177\n",
            "Test RMSE: 2.2089\n",
            "Test R2: -3.9776\n",
            "Downloading adult.data...\n",
            "Downloading adult.test...\n",
            "\n",
            "--- Running on Adult Dataset (Classification) ---\n",
            "\n",
            "--- Starting grid search for Vanilla RVFL on Adult Income (4 combinations) ---\n",
            "  Trying 1/4: Params={'N': 10, 'C': 0.01}\n",
            "    Accuracy: 0.8091\n",
            "  Trying 2/4: Params={'N': 10, 'C': 1.0}\n",
            "    Accuracy: 0.8143\n",
            "  Trying 3/4: Params={'N': 50, 'C': 0.01}\n",
            "    Accuracy: 0.8279\n",
            "  Trying 4/4: Params={'N': 50, 'C': 1.0}\n",
            "    Accuracy: 0.8322\n",
            "\n",
            "Best Hyperparameters for Vanilla RVFL on Adult Income:\n",
            "{'N': 50, 'C': 1.0}\n",
            "Test Accuracy: 0.8306\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting grid search for SP-RVFL on Adult Income (4 combinations) ---\n",
            "  Trying 1/4: Params={'N': 10, 'C': 0.01, 'PT': 10000.0}\n",
            "    Accuracy: 0.8119\n",
            "  Trying 2/4: Params={'N': 10, 'C': 1.0, 'PT': 10000.0}\n",
            "    Accuracy: 0.8174\n",
            "  Trying 3/4: Params={'N': 50, 'C': 0.01, 'PT': 10000.0}\n",
            "    Accuracy: 0.8288\n",
            "  Trying 4/4: Params={'N': 50, 'C': 1.0, 'PT': 10000.0}\n",
            "    Accuracy: 0.8266\n",
            "\n",
            "Best Hyperparameters for SP-RVFL on Adult Income:\n",
            "{'N': 50, 'C': 0.01, 'PT': 10000.0}\n",
            "Test Accuracy: 0.8315\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting grid search for SW-SP-RVFL (Dynamic) on Adult Income (8 combinations) ---\n",
            "  Trying 1/8: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0}\n",
            "    Accuracy: 0.8244\n",
            "  Trying 2/8: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5}\n",
            "    Accuracy: 0.8135\n",
            "  Trying 3/8: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0}\n",
            "    Accuracy: 0.8249\n",
            "  Trying 4/8: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5}\n",
            "    Accuracy: 0.8124\n",
            "  Trying 5/8: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0}\n",
            "    Accuracy: 0.8430\n",
            "  Trying 6/8: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5}\n",
            "    Accuracy: 0.8390\n",
            "  Trying 7/8: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0}\n",
            "    Accuracy: 0.8346\n",
            "  Trying 8/8: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5}\n",
            "    Accuracy: 0.8306\n",
            "\n",
            "Best Hyperparameters for SW-SP-RVFL (Dynamic) on Adult Income:\n",
            "{'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0}\n",
            "Test Accuracy: 0.8364\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting grid search for SW-SP Hidden + Direct (Dynamic) on Adult Income (8 combinations) ---\n",
            "  Trying 1/8: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5}\n",
            "    Accuracy: 0.8226\n",
            "  Trying 2/8: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5}\n",
            "    Accuracy: 0.8100\n",
            "  Trying 3/8: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5}\n",
            "    Accuracy: 0.8115\n",
            "  Trying 4/8: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5}\n",
            "    Accuracy: 0.8134\n",
            "  Trying 5/8: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5}\n",
            "    Accuracy: 0.8425\n",
            "  Trying 6/8: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5}\n",
            "    Accuracy: 0.8313\n",
            "  Trying 7/8: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5}\n",
            "    Accuracy: 0.8338\n",
            "  Trying 8/8: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5}\n",
            "    Accuracy: 0.8402\n",
            "\n",
            "Best Hyperparameters for SW-SP Hidden + Direct (Dynamic) on Adult Income:\n",
            "{'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5}\n",
            "Test Accuracy: 0.8370\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting grid search for SA-SW-SP-RVFL (Pre-trained + Dynamic) on Adult Income (16 combinations) ---\n",
            "  Trying 1/16: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=10, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.2642\n",
            "AE Epoch 20/20, Loss: 1.2205\n",
            "    Accuracy: 0.8101\n",
            "  Trying 2/16: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=10, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.2821\n",
            "AE Epoch 20/20, Loss: 1.2407\n",
            "    Accuracy: 0.8129\n",
            "  Trying 3/16: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=10, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.2144\n",
            "AE Epoch 20/20, Loss: 1.1730\n",
            "    Accuracy: 0.8096\n",
            "  Trying 4/16: Params={'N': 10, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=10, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.2016\n",
            "AE Epoch 20/20, Loss: 1.1688\n",
            "    Accuracy: 0.8191\n",
            "  Trying 5/16: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=10, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.2667\n",
            "AE Epoch 20/20, Loss: 1.2165\n",
            "    Accuracy: 0.7991\n",
            "  Trying 6/16: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=10, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.2702\n",
            "AE Epoch 20/20, Loss: 1.2348\n",
            "    Accuracy: 0.7970\n",
            "  Trying 7/16: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=10, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.2683\n",
            "AE Epoch 20/20, Loss: 1.2115\n",
            "    Accuracy: 0.8214\n",
            "  Trying 8/16: Params={'N': 10, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=10, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.3310\n",
            "AE Epoch 20/20, Loss: 1.2914\n",
            "    Accuracy: 0.8155\n",
            "  Trying 9/16: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=50, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.0603\n",
            "AE Epoch 20/20, Loss: 1.0070\n",
            "    Accuracy: 0.8183\n",
            "  Trying 10/16: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=50, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.5676\n",
            "AE Epoch 20/20, Loss: 1.4323\n",
            "    Accuracy: 0.8149\n",
            "  Trying 11/16: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=50, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.3563\n",
            "AE Epoch 20/20, Loss: 1.1674\n",
            "    Accuracy: 0.8156\n",
            "  Trying 12/16: Params={'N': 50, 'C': 0.01, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=50, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.5130\n",
            "AE Epoch 20/20, Loss: 1.3801\n",
            "    Accuracy: 0.8175\n",
            "  Trying 13/16: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=50, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.2286\n",
            "AE Epoch 20/20, Loss: 1.0981\n",
            "    Accuracy: 0.8162\n",
            "  Trying 14/16: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.0, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.01, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=50, lambda_l1=0.01, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.5177\n",
            "AE Epoch 20/20, Loss: 1.4151\n",
            "    Accuracy: 0.8173\n",
            "  Trying 15/16: Params={'N': 50, 'C': 1.0, 'PT': 10000.0, 'importance_bias': 0.5, 'sparsity_ratio_direct': 0.5, 'sa_lambda_l1': 0.001, 'sa_epochs': 20}\n",
            "  Pre-training Sparse Autoencoder with input_dim=14, hidden_dim=50, lambda_l1=0.001, epochs=20...\n",
            "AE Epoch 10/20, Loss: 1.3238\n",
            "AE Epoch 20/20, Loss: 1.1803\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, r2_score\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_digits, load_diabetes, load_wine, fetch_california_housing\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier # For feature importance\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# --- Activation ---\n",
        "def activation(x, task):\n",
        "    return 1 / (1 + np.exp(-x)) if task == 'classification' else np.maximum(0, x)\n",
        "\n",
        "# --- Graph Mask Generator ---\n",
        "# This generates a 2D mask (features x hidden_nodes) for the W matrix\n",
        "def generate_small_world_mask(n_features, n_hidden, k=4, p=0.3):\n",
        "    if n_features == 0 or n_hidden == 0:\n",
        "        return np.zeros((n_features, n_hidden))\n",
        "\n",
        "    k_adjusted = min(k, n_features - 1)\n",
        "    if k_adjusted <= 0 and n_features > 1: k_adjusted = 1\n",
        "    elif n_features == 1 and k_adjusted == 0: k_adjusted = 1 # special case for single feature graph\n",
        "\n",
        "    G = nx.watts_strogatz_graph(n=n_features, k=k_adjusted, p=p)\n",
        "    mask = np.zeros((n_features, n_hidden))\n",
        "    for h in range(n_hidden):\n",
        "        center = np.random.choice(n_features)\n",
        "        neighbors = list(G.neighbors(center)) + [center]\n",
        "        mask[neighbors, h] = 1\n",
        "    return mask\n",
        "\n",
        "# --- Function to generate a 1D small-world feature mask for direct connections ---\n",
        "def generate_small_world_feature_mask(n_features, k=4, p=0.3, sparsity_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Generates a 1D mask for input features based on small-world connectivity ideas.\n",
        "    It identifies 'important' features based on a small-world graph,\n",
        "    resulting in a binary mask (1 for selected, 0 for not).\n",
        "    \"\"\"\n",
        "    if n_features == 0:\n",
        "        return np.array([])\n",
        "    if n_features == 1:\n",
        "        return np.array([1.0])\n",
        "\n",
        "    k_adjusted = min(k, n_features - 1)\n",
        "    if k_adjusted <= 0 and n_features > 1:\n",
        "         k_adjusted = 1 # Minimum k for Watts-Strogatz\n",
        "\n",
        "    G = nx.watts_strogatz_graph(n=n_features, k=k_adjusted, p=p)\n",
        "    mask = np.zeros(n_features)\n",
        "\n",
        "    selected_features_indices = set()\n",
        "    num_to_select = max(1, int(n_features * (1.0 - sparsity_ratio))) # Target number of active features\n",
        "\n",
        "    attempts = 0\n",
        "    max_attempts = n_features * 5 # Limit attempts to prevent infinite loop for sparse graphs\n",
        "\n",
        "    while len(selected_features_indices) < num_to_select and attempts < max_attempts:\n",
        "        center = np.random.choice(n_features)\n",
        "        neighbors = list(G.neighbors(center)) + [center]\n",
        "        for n in neighbors:\n",
        "            selected_features_indices.add(n)\n",
        "            if len(selected_features_indices) >= num_to_select:\n",
        "                break\n",
        "        attempts += 1\n",
        "\n",
        "    mask[list(selected_features_indices)] = 1\n",
        "    return mask\n",
        "\n",
        "\n",
        "# --- Dynamic Mask Generation based on Feature Importance ---\n",
        "def generate_dynamic_small_world_mask_fi(n_features, n_nodes, k=4, p=0.3, feature_importances=None, importance_bias=0.2):\n",
        "    \"\"\"\n",
        "    Generates a small-world inspired mask, biasing connection centers towards more important features.\n",
        "\n",
        "    n_nodes: Can be n_hidden for hidden layer, or n_features for direct layer.\n",
        "    feature_importances: 1D array of importance scores for each feature.\n",
        "    importance_bias: How much to bias the selection of the 'center' feature.\n",
        "                     0 = no bias (random), 1 = always pick most important.\n",
        "    \"\"\"\n",
        "    if n_features == 0 or n_nodes == 0:\n",
        "        return np.zeros((n_features, n_nodes))\n",
        "\n",
        "    k_adjusted = min(k, n_features - 1)\n",
        "    if k_adjusted <= 0 and n_features > 1: k_adjusted = 1\n",
        "    elif n_features == 1 and k_adjusted == 0: k_adjusted = 1\n",
        "\n",
        "    G = nx.watts_strogatz_graph(n=n_features, k=k_adjusted, p=p)\n",
        "    mask = np.zeros((n_features, n_nodes))\n",
        "\n",
        "    if feature_importances is None or np.sum(feature_importances) == 0:\n",
        "        # Fallback to standard small-world if no importances or all are zero\n",
        "        for h in range(n_nodes):\n",
        "            center = np.random.choice(n_features)\n",
        "            neighbors = list(G.neighbors(center)) + [center]\n",
        "            mask[neighbors, h] = 1\n",
        "    else:\n",
        "        # Normalize importances to probabilities for biased sampling\n",
        "        probs = feature_importances / (np.sum(feature_importances) + 1e-8) # Add epsilon to avoid div by zero\n",
        "\n",
        "        # Adjust probabilities to bias towards higher importance\n",
        "        uniform_probs = np.ones(n_features) / n_features\n",
        "        biased_probs = (1 - importance_bias) * uniform_probs + importance_bias * probs\n",
        "        biased_probs /= (np.sum(biased_probs) + 1e-8) # Re-normalize\n",
        "\n",
        "        for h in range(n_nodes):\n",
        "            center = np.random.choice(n_features, p=biased_probs) # Biased choice\n",
        "            neighbors = list(G.neighbors(center)) + [center]\n",
        "            mask[neighbors, h] = 1\n",
        "    return mask\n",
        "\n",
        "def generate_dynamic_small_world_feature_mask_fi(n_features, k=4, p=0.3, sparsity_ratio=0.5, feature_importances=None, importance_bias=0.2):\n",
        "    \"\"\"\n",
        "    Generates a 1D feature mask, biasing selection towards more important features.\n",
        "    \"\"\"\n",
        "    if n_features == 0:\n",
        "        return np.array([])\n",
        "    if n_features == 1:\n",
        "        return np.array([1.0])\n",
        "\n",
        "    k_adjusted = min(k, n_features - 1)\n",
        "    if k_adjusted <= 0: k_adjusted = 1\n",
        "\n",
        "    G = nx.watts_strogatz_graph(n=n_features, k=k_adjusted, p=p)\n",
        "    mask = np.zeros(n_features)\n",
        "\n",
        "    selected_features_indices = set()\n",
        "    num_to_select = max(1, int(n_features * (1.0 - sparsity_ratio)))\n",
        "\n",
        "    if feature_importances is None or np.sum(feature_importances) == 0:\n",
        "        biased_probs = np.ones(n_features) / n_features # Uniform if no importances\n",
        "    else:\n",
        "        probs = feature_importances / (np.sum(feature_importances) + 1e-8)\n",
        "        uniform_probs = np.ones(n_features) / n_features\n",
        "        biased_probs = (1 - importance_bias) * uniform_probs + importance_bias * probs\n",
        "        biased_probs /= (np.sum(biased_probs) + 1e-8) # Re-normalize\n",
        "\n",
        "    attempts = 0\n",
        "    max_attempts = n_features * 5\n",
        "\n",
        "    while len(selected_features_indices) < num_to_select and attempts < max_attempts:\n",
        "        center = np.random.choice(n_features, p=biased_probs) # Biased choice\n",
        "        neighbors = list(G.neighbors(center)) + [center]\n",
        "        for n in neighbors:\n",
        "            selected_features_indices.add(n)\n",
        "            if len(selected_features_indices) >= num_to_select:\n",
        "                break\n",
        "        attempts += 1\n",
        "\n",
        "    mask[list(selected_features_indices)] = 1\n",
        "    return mask\n",
        "\n",
        "\n",
        "# --- Bayesian Solver (ADAM-based) ---\n",
        "def solve_mu_adam(H, Y, alpha, sigma2=1.0, lr=1e-2, beta1=0.9, beta2=0.999, eps=1e-8,\n",
        "                  max_iter=1000, tol=1e-6):\n",
        "    N, D = H.shape\n",
        "    K = Y.shape[1]\n",
        "\n",
        "    if D == 0: # Handle case where H has no features\n",
        "        return np.zeros((0, K))\n",
        "\n",
        "    Mu = np.random.randn(D, K) * 0.01\n",
        "    m = np.zeros_like(Mu)\n",
        "    v = np.zeros_like(Mu)\n",
        "\n",
        "    for t in range(1, max_iter + 1):\n",
        "        pred = H @ Mu\n",
        "        error = pred - Y\n",
        "\n",
        "        # Gradient with L2 regularization from alpha (precision parameter)\n",
        "        grad = (H.T @ error) / sigma2 + np.diag(alpha) @ Mu\n",
        "\n",
        "        m = beta1 * m + (1 - beta1) * grad\n",
        "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
        "\n",
        "        m_hat = m / (1 - beta1 ** t)\n",
        "        v_hat = v / (1 - beta2 ** t)\n",
        "\n",
        "        Mu_new = Mu - lr * m_hat / (np.sqrt(v_hat) + eps)\n",
        "\n",
        "        if np.linalg.norm(Mu_new - Mu) < tol * np.linalg.norm(Mu):\n",
        "            break\n",
        "        Mu = Mu_new\n",
        "\n",
        "    return Mu\n",
        "\n",
        "# --- SP-RVFL Iterative Solver (with H pruning as per provided algorithm) ---\n",
        "def sp_rvfl_solver_iterative(H_initial, Y, max_iter=30, tol=1e-4, pruning_threshold=1e4, outer_iters=1):\n",
        "    N, D = H_initial.shape\n",
        "    K = Y.shape[1]\n",
        "\n",
        "    if D == 0: # Handle case where H has no features\n",
        "        return np.zeros((0, K)), np.array([])\n",
        "\n",
        "    H_current = H_initial.copy() # H will be modified in place in outer loop\n",
        "    current_mask = np.ones(D) # Mask for the entire Z matrix columns\n",
        "\n",
        "    for outer in range(outer_iters):\n",
        "        alpha = np.ones(D) # Initialize alpha for current iteration\n",
        "        sigma2 = 1.0 # Initialize sigma2 for current iteration\n",
        "        eps = 1e-6\n",
        "\n",
        "        # If all features are pruned, stop early\n",
        "        if np.all(current_mask == 0) and N > 0:\n",
        "            Mu = np.zeros((D, K))\n",
        "            break # Exit outer loop\n",
        "\n",
        "        # Inner loop for ARD parameter estimation\n",
        "        for _ in range(max_iter):\n",
        "            # Estimate Mu using ADAM optimizer on the currently masked H\n",
        "            Mu = solve_mu_adam(H_current, Y, alpha, sigma2, lr=1e-2)\n",
        "\n",
        "            if Mu.shape[0] == 0: # If solve_mu_adam decided there are no features left\n",
        "                break\n",
        "\n",
        "            # Gamma and alpha update (Approximation for ARD)\n",
        "            gamma = np.ones(D) # Treating all as active for gamma estimation\n",
        "            mu_squared_sum = np.sum(Mu**2, axis=1) # Sum of squares of coefficients for each feature\n",
        "            alpha_new = gamma / (mu_squared_sum + eps) # Update alpha (precision)\n",
        "\n",
        "            # Update sigma2 (noise precision)\n",
        "            denom = N * K - np.sum(gamma) # Degrees of freedom\n",
        "            denom = max(denom, eps) # Ensure denominator is not zero\n",
        "            sigma2_new = np.sum((Y - H_current @ Mu) ** 2) / denom\n",
        "\n",
        "            # Check for convergence\n",
        "            if np.linalg.norm(alpha - alpha_new) < tol and abs(sigma2 - sigma2_new) < tol * sigma2:\n",
        "                break\n",
        "\n",
        "            alpha, sigma2 = alpha_new, sigma2_new\n",
        "\n",
        "        # Prune columns of H for the next outer iteration based on alpha\n",
        "        new_mask = (alpha < pruning_threshold).astype(float)\n",
        "\n",
        "        # Update the overall mask and apply to H_current for the next outer iteration\n",
        "        current_mask *= new_mask\n",
        "        H_current = H_initial * current_mask # Apply the cumulative mask to the original H_initial\n",
        "                                             # This effectively re-masks the initial H.\n",
        "\n",
        "    # Final Mu is computed using the final pruned H (H_initial * current_mask)\n",
        "    if np.all(current_mask == 0) and N > 0: # If all features were pruned\n",
        "         Mu = np.zeros((D, K))\n",
        "    else:\n",
        "        Mu = solve_mu_adam(H_initial * current_mask, Y, alpha, sigma2, lr=1e-2) # Use the final alpha and sigma2\n",
        "\n",
        "    return Mu, current_mask # Return the final beta and the learned mask\n",
        "\n",
        "\n",
        "# --- Preprocessing ---\n",
        "def preprocess_classification(X, y):\n",
        "    # Ensure y is 1D for stratification if it's currently (N, 1)\n",
        "    y_stratify = y.flatten() if y.ndim > 1 else y\n",
        "    # Stratify only if there are multiple classes\n",
        "    do_stratify = len(np.unique(y_stratify)) > 1\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,\n",
        "                                                        stratify=y_stratify if do_stratify else None)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Handle both binary and multi-class classification\n",
        "    if y.ndim == 1 or (y.ndim == 2 and y.shape[1] == 1):\n",
        "        if len(np.unique(y_stratify)) > 2: # Multi-class\n",
        "            encoder = OneHotEncoder(sparse_output=False)\n",
        "            y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "            y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "            # Keep original y_train/y_test for inverse transform if needed for metrics (e.g., confusion matrix labels)\n",
        "            y_train_raw = y_train.flatten()\n",
        "            y_test_raw = y_test.flatten()\n",
        "        else: # Binary, treat as single column output for the model\n",
        "            encoder = LabelEncoder()\n",
        "            y_train_raw = encoder.fit_transform(y_train.flatten()) # Store original labels\n",
        "            y_test_raw = encoder.transform(y_test.flatten())\n",
        "            y_train_onehot = y_train_raw.reshape(-1, 1) # Model expects (N, 1) or (N, K)\n",
        "            y_test_onehot = y_test_raw.reshape(-1, 1)\n",
        "            encoder = None # No one-hot encoder for binary as y is already 0/1 after LabelEncoder\n",
        "    else: # Already one-hot encoded (unlikely for raw datasets but for robustness)\n",
        "        encoder = None\n",
        "        y_train_onehot = y_train\n",
        "        y_test_onehot = y_test\n",
        "        y_train_raw = np.argmax(y_train, axis=1) if y_train.shape[1] > 1 else y_train.flatten()\n",
        "        y_test_raw = np.argmax(y_test, axis=1) if y_test.shape[1] > 1 else y_test.flatten()\n",
        "\n",
        "    return X_train, X_test, y_train_onehot, y_test_onehot, encoder, y_train_raw, y_test_raw\n",
        "\n",
        "def preprocess_regression(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train = scaler_X.fit_transform(X_train)\n",
        "    X_test = scaler_X.transform(X_test)\n",
        "    y_train = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
        "    y_test = scaler_y.transform(y_test.reshape(-1, 1))\n",
        "    return X_train, X_test, y_train, y_test, scaler_y, y_train, y_test # y_train_raw, y_test_raw not relevant for regression\n",
        "\n",
        "\n",
        "# --- Sparse Autoencoder for Pre-training W and b ---\n",
        "class SparseAutoencoder:\n",
        "    def __init__(self, input_dim, hidden_dim, lambda_l1=0.01, lr=1e-3, epochs=50, task='classification'):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lambda_l1 = lambda_l1\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.task = task # Used for activation function consistency\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W_encoder = np.random.randn(input_dim, hidden_dim) * np.sqrt(2. / (input_dim + hidden_dim))\n",
        "        self.b_hidden = np.zeros(hidden_dim)\n",
        "        self.W_decoder = np.random.randn(hidden_dim, input_dim) * np.sqrt(2. / (input_dim + hidden_dim))\n",
        "        self.b_output = np.zeros(input_dim)\n",
        "\n",
        "        # ADAM optimizer parameters\n",
        "        self.m_W_enc, self.v_W_enc = np.zeros_like(self.W_encoder), np.zeros_like(self.W_encoder)\n",
        "        self.m_b_hid, self.v_b_hid = np.zeros_like(self.b_hidden), np.zeros_like(self.b_hidden)\n",
        "        self.m_W_dec, self.v_W_dec = np.zeros_like(self.W_decoder), np.zeros_like(self.W_decoder)\n",
        "        self.m_b_out, self.v_b_out = np.zeros_like(self.b_output), np.zeros_like(self.b_output)\n",
        "        self.t = 0 # Timestep for Adam\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.eps = 1e-8\n",
        "\n",
        "\n",
        "    def _adam_update(self, param, grad, m, v, t):\n",
        "        m = self.beta1 * m + (1 - self.beta1) * grad\n",
        "        v = self.beta2 * v + (1 - self.beta2) * (grad ** 2)\n",
        "        m_hat = m / (1 - self.beta1 ** t)\n",
        "        v_hat = v / (1 - self.beta2 ** t)\n",
        "        param_new = param - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "        return param_new, m, v\n",
        "\n",
        "    def fit(self, X, small_world_mask=None, verbose=False):\n",
        "        if X.shape[1] == 0:\n",
        "            print(\"Warning: Input X for Autoencoder has no features. Skipping pre-training.\")\n",
        "            return\n",
        "\n",
        "        # Apply small-world mask to encoder weights initially\n",
        "        if small_world_mask is not None:\n",
        "            if small_world_mask.shape != self.W_encoder.shape:\n",
        "                raise ValueError(f\"Small-world mask shape {small_world_mask.shape} does not match W_encoder shape {self.W_encoder.shape}\")\n",
        "            self.W_encoder *= small_world_mask\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.t += 1 # Update Adam timestep\n",
        "\n",
        "            # Forward pass\n",
        "            hidden_layer = activation(X @ self.W_encoder + self.b_hidden, self.task)\n",
        "            reconstruction = X @ self.W_encoder + self.b_hidden # Using linear output for autoencoder (common for reconstruction)\n",
        "            reconstruction_decoder = hidden_layer @ self.W_decoder + self.b_output\n",
        "\n",
        "            # Loss (MSE + L1 on W_encoder)\n",
        "            reconstruction_error = reconstruction_decoder - X\n",
        "            loss = np.mean(reconstruction_error**2) + self.lambda_l1 * np.sum(np.abs(self.W_encoder))\n",
        "\n",
        "            # Backpropagation (simplified for linear autoencoder, chain rule for activation is crucial)\n",
        "            # Derivative of reconstruction_decoder w.r.t. W_decoder\n",
        "            grad_W_decoder = hidden_layer.T @ reconstruction_error / X.shape[0]\n",
        "            grad_b_output = np.sum(reconstruction_error, axis=0) / X.shape[0]\n",
        "\n",
        "            # Derivative of reconstruction_decoder w.r.t. hidden_layer\n",
        "            grad_hidden_layer = reconstruction_error @ self.W_decoder.T\n",
        "\n",
        "            # Derivative of hidden_layer (activation(Z)) w.r.t. Z = activation_derivative(Z)\n",
        "            # Derivative of Z (X @ W_encoder + b_hidden) w.r.t. W_encoder and b_hidden\n",
        "\n",
        "            # Assuming 'activation' for hidden layer is sigmoid or ReLU.\n",
        "            # If sigmoid: act_deriv = h * (1 - h)\n",
        "            # If ReLU: act_deriv = (h > 0).astype(float)\n",
        "\n",
        "            # For activation=sigmoid:\n",
        "            if self.task == 'classification': # Sigmoid for hidden\n",
        "                 hidden_layer_input = X @ self.W_encoder + self.b_hidden\n",
        "                 act_deriv = hidden_layer * (1 - hidden_layer) # Derivative of sigmoid\n",
        "                 grad_W_encoder = X.T @ (grad_hidden_layer * act_deriv) / X.shape[0] + self.lambda_l1 * np.sign(self.W_encoder)\n",
        "                 grad_b_hidden = np.sum(grad_hidden_layer * act_deriv, axis=0) / X.shape[0]\n",
        "            else: # ReLU for hidden\n",
        "                 hidden_layer_input = X @ self.W_encoder + self.b_hidden\n",
        "                 act_deriv = (hidden_layer_input > 0).astype(float) # Derivative of ReLU\n",
        "                 grad_W_encoder = X.T @ (grad_hidden_layer * act_deriv) / X.shape[0] + self.lambda_l1 * np.sign(self.W_encoder)\n",
        "                 grad_b_hidden = np.sum(grad_hidden_layer * act_deriv, axis=0) / X.shape[0]\n",
        "\n",
        "\n",
        "            # ADAM updates\n",
        "            self.W_encoder, self.m_W_enc, self.v_W_enc = self._adam_update(self.W_encoder, grad_W_encoder, self.m_W_enc, self.v_W_enc, self.t)\n",
        "            self.b_hidden, self.m_b_hid, self.v_b_hid = self._adam_update(self.b_hidden, grad_b_hidden, self.m_b_hid, self.v_b_hid, self.t)\n",
        "            self.W_decoder, self.m_W_dec, self.v_W_dec = self._adam_update(self.W_decoder, grad_W_decoder, self.m_W_dec, self.v_W_dec, self.t)\n",
        "            self.b_output, self.m_b_out, self.v_b_out = self._adam_update(self.b_output, grad_b_output, self.m_b_out, self.v_b_out, self.t)\n",
        "\n",
        "            # Re-apply small-world mask to W_encoder after update (projection onto the allowed connections)\n",
        "            if small_world_mask is not None:\n",
        "                self.W_encoder *= small_world_mask\n",
        "\n",
        "            if verbose and (epoch + 1) % 10 == 0:\n",
        "                print(f\"AE Epoch {epoch+1}/{self.epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def get_rvfl_params(self):\n",
        "        # Return the encoder weights and hidden biases for RVFL\n",
        "        return self.W_encoder, self.b_hidden\n",
        "\n",
        "\n",
        "# --- Core RVFL ---\n",
        "def base_rvfl(X_train, y_train, X_test, n_hidden=256, W=None, b=None,\n",
        "              use_sp=False, mask=None, mask_direct=None, C=1e-3, task='classification',\n",
        "              pruning_threshold=1e4):\n",
        "\n",
        "    n_features = X_train.shape[1]\n",
        "\n",
        "    if n_features == 0: # Handle cases where input features are empty\n",
        "        print(\"Warning: No input features (X_train.shape[1] is 0). Returning zero predictions.\")\n",
        "        return np.zeros((X_test.shape[0], y_train.shape[1]))\n",
        "\n",
        "    if W is None:\n",
        "        W = np.random.randn(n_features, n_hidden)\n",
        "        if mask is not None:\n",
        "            W *= mask # Element-wise multiplication for mask\n",
        "    if b is None:\n",
        "        b = np.random.randn(n_hidden)\n",
        "\n",
        "    H_train = activation(X_train @ W + b, task)\n",
        "    H_test = activation(X_test @ W + b, task)\n",
        "\n",
        "    X_train_proj = X_train\n",
        "    X_test_proj = X_test\n",
        "\n",
        "    # Apply direct mask (expected to be 1D for feature selection)\n",
        "    if mask_direct is not None and mask_direct.ndim == 1 and mask_direct.shape[0] == n_features:\n",
        "        X_train_proj = X_train * mask_direct\n",
        "        X_test_proj = X_test * mask_direct\n",
        "    elif mask_direct is not None:\n",
        "        print(f\"Warning: mask_direct has unexpected shape {mask_direct.shape}. Expected (n_features,). Not applying direct mask based on this.\")\n",
        "\n",
        "    Z_train = np.hstack([X_train_proj, H_train])\n",
        "    Z_test = np.hstack([X_test_proj, H_test])\n",
        "\n",
        "    if Z_train.shape[1] == 0:\n",
        "        print(\"Warning: Z_train has 0 columns after stacking and masking. Cannot train model. Returning zero predictions.\")\n",
        "        return np.zeros((X_test.shape[0], y_train.shape[1]))\n",
        "\n",
        "    if use_sp:\n",
        "        beta, learned_mask = sp_rvfl_solver_iterative(Z_train, y_train, pruning_threshold=pruning_threshold)\n",
        "\n",
        "        # Apply the learned_mask (from Z_train) to Z_test for consistent feature selection\n",
        "        # This mask is a 1D array indicating which columns of Z are kept/pruned.\n",
        "        Z_test_masked = Z_test * learned_mask\n",
        "\n",
        "        # A sanity check for dimension consistency for matrix multiplication\n",
        "        if Z_test_masked.shape[1] != beta.shape[0]:\n",
        "            print(f\"Dimension mismatch after pruning: Z_test_masked columns {Z_test_masked.shape[1]}, beta rows {beta.shape[0]}. Attempting to adjust.\")\n",
        "            # This should ideally not happen if learned_mask is correctly derived from Z_train.\n",
        "            # If it does, it implies an issue in how Z_train/Z_test were constructed or mask applied.\n",
        "            if Z_test_masked.shape[1] > beta.shape[0]:\n",
        "                Z_test_masked = Z_test_masked[:, :beta.shape[0]] # Truncate Z_test_masked\n",
        "            else: # Z_test_masked.shape[1] < beta.shape[0]\n",
        "                padding_cols = beta.shape[0] - Z_test_masked.shape[1]\n",
        "                Z_test_masked = np.hstack([Z_test_masked, np.zeros((Z_test_masked.shape[0], padding_cols))])\n",
        "\n",
        "        return Z_test_masked @ beta\n",
        "    else:\n",
        "        # Regularized least squares\n",
        "        # Adding a small identity to ensure invertibility, especially if D > N\n",
        "        beta = np.linalg.inv(Z_train.T @ Z_train + C * np.eye(Z_train.shape[1])) @ Z_train.T @ y_train\n",
        "        return Z_test @ beta\n",
        "\n",
        "# --- Specialized RVFL Variants ---\n",
        "def vanilla_rvfl(X_train, y_train, X_test, n_hidden=256, C=1e-3, task='classification', pruning_threshold=1e4):\n",
        "    # Note: pruning_threshold is passed but ignored by base_rvfl if use_sp is False\n",
        "    return base_rvfl(X_train, y_train, X_test, n_hidden=n_hidden, C=C, task=task, use_sp=False, pruning_threshold=pruning_threshold)\n",
        "\n",
        "def sp_rvfl(X_train, y_train, X_test, n_hidden=256, C=1e-3, task='classification', pruning_threshold=1e4):\n",
        "    # This function explicitly sets use_sp=True for base_rvfl\n",
        "    return base_rvfl(X_train, y_train, X_test, n_hidden=n_hidden, use_sp=True, C=C, task=task, pruning_threshold=pruning_threshold)\n",
        "\n",
        "def sw_sp_rvfl(X_train, y_train, X_test, n_hidden=256, k=4, p=0.3, C=1e-3, task='classification', pruning_threshold=1e4, importance_bias=0.2):\n",
        "    # Calculate feature importances using a simple RF model\n",
        "    # Ensure y_train is 1D for RF fit if it's (N,1)\n",
        "    y_fit = y_train.flatten() if y_train.ndim > 1 and y_train.shape[1] == 1 else y_train\n",
        "\n",
        "    if X_train.shape[1] == 0:\n",
        "        feature_importances = np.array([])\n",
        "    elif X_train.shape[1] == 1:\n",
        "        feature_importances = np.array([1.0])\n",
        "    else:\n",
        "        if task == 'classification':\n",
        "            fi_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        else:\n",
        "            fi_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "        # Handle cases where y_fit is single-valued (e.g., all 0s or 1s after split)\n",
        "        # RF will complain if only one class is present in y_fit\n",
        "        if task == 'classification' and len(np.unique(y_fit)) < 2:\n",
        "            # print(\"Warning: Only one class present in y_train for RandomForest. Feature importances will be uniform.\")\n",
        "            feature_importances = np.ones(X_train.shape[1]) / X_train.shape[1]\n",
        "        else:\n",
        "            fi_model.fit(X_train, y_fit)\n",
        "            feature_importances = fi_model.feature_importances_\n",
        "\n",
        "    mask = generate_dynamic_small_world_mask_fi(X_train.shape[1], n_hidden, k, p,\n",
        "                                                feature_importances=feature_importances,\n",
        "                                                importance_bias=importance_bias)\n",
        "\n",
        "    return base_rvfl(X_train, y_train, X_test, n_hidden=n_hidden, use_sp=True, mask=mask, C=C, task=task, pruning_threshold=pruning_threshold)\n",
        "\n",
        "def sw_sp_hidden_direct(X_train, y_train, X_test, n_hidden=256, k=4, p=0.3, C=1e-3, task='classification', pruning_threshold=1e4, importance_bias=0.2, sparsity_ratio_direct=0.5):\n",
        "    # Calculate feature importances\n",
        "    y_fit = y_train.flatten() if y_train.ndim > 1 and y_train.shape[1] == 1 else y_train\n",
        "\n",
        "    if X_train.shape[1] == 0:\n",
        "        feature_importances = np.array([])\n",
        "    elif X_train.shape[1] == 1:\n",
        "        feature_importances = np.array([1.0])\n",
        "    else:\n",
        "        if task == 'classification':\n",
        "            fi_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        else:\n",
        "            fi_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "        if task == 'classification' and len(np.unique(y_fit)) < 2:\n",
        "            # print(\"Warning: Only one class present in y_train for RandomForest. Feature importances will be uniform.\")\n",
        "            feature_importances = np.ones(X_train.shape[1]) / X_train.shape[1]\n",
        "        else:\n",
        "            fi_model.fit(X_train, y_fit)\n",
        "            feature_importances = fi_model.feature_importances_\n",
        "\n",
        "    mask_hidden = generate_dynamic_small_world_mask_fi(X_train.shape[1], n_hidden, k, p,\n",
        "                                                       feature_importances=feature_importances,\n",
        "                                                       importance_bias=importance_bias)\n",
        "\n",
        "    # Generate 1D direct mask using dynamic importance\n",
        "    mask_direct_1d = generate_dynamic_small_world_feature_mask_fi(X_train.shape[1], k, p,\n",
        "                                                                sparsity_ratio=sparsity_ratio_direct,\n",
        "                                                                feature_importances=feature_importances,\n",
        "                                                                importance_bias=importance_bias)\n",
        "\n",
        "    return base_rvfl(X_train, y_train, X_test, n_hidden=n_hidden, use_sp=True,\n",
        "                     mask=mask_hidden, mask_direct=mask_direct_1d, C=C, task=task,\n",
        "                     pruning_threshold=pruning_threshold)\n",
        "\n",
        "# --- NEW RVFL Variant with Sparse Autoencoder Pre-training ---\n",
        "def sa_sw_sp_rvfl(X_train, y_train, X_test, n_hidden=256, k=4, p=0.3, C=1e-3, task='classification',\n",
        "                  pruning_threshold=1e4, importance_bias=0.2, sparsity_ratio_direct=0.5,\n",
        "                  sa_lambda_l1=0.01, sa_epochs=50):\n",
        "\n",
        "    n_features = X_train.shape[1]\n",
        "    if n_features == 0:\n",
        "        print(\"Warning: No input features for SA-SW-SP-RVFL. Returning zero predictions.\")\n",
        "        return np.zeros((X_test.shape[0], y_train.shape[1]))\n",
        "\n",
        "    # Step 1: Calculate feature importances for mask generation\n",
        "    y_fit = y_train.flatten() if y_train.ndim > 1 and y_train.shape[1] == 1 else y_train\n",
        "    if X_train.shape[1] == 1:\n",
        "        feature_importances = np.array([1.0])\n",
        "    else:\n",
        "        if task == 'classification':\n",
        "            fi_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        else:\n",
        "            fi_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "        if task == 'classification' and len(np.unique(y_fit)) < 2:\n",
        "            feature_importances = np.ones(X_train.shape[1]) / X_train.shape[1]\n",
        "        else:\n",
        "            fi_model.fit(X_train, y_fit)\n",
        "            feature_importances = fi_model.feature_importances_\n",
        "\n",
        "    # Step 2: Generate small-world mask for the autoencoder's hidden layer weights (W_encoder)\n",
        "    # This ensures the autoencoder itself learns sparse, small-world structured weights\n",
        "    mask_sa_W_encoder = generate_dynamic_small_world_mask_fi(n_features, n_hidden, k, p,\n",
        "                                                            feature_importances=feature_importances,\n",
        "                                                            importance_bias=importance_bias)\n",
        "\n",
        "    # Step 3: Pre-train the Sparse Autoencoder\n",
        "    print(f\"  Pre-training Sparse Autoencoder with input_dim={n_features}, hidden_dim={n_hidden}, lambda_l1={sa_lambda_l1}, epochs={sa_epochs}...\")\n",
        "    sa = SparseAutoencoder(input_dim=n_features, hidden_dim=n_hidden,\n",
        "                           lambda_l1=sa_lambda_l1, epochs=sa_epochs, task=task)\n",
        "    sa.fit(X_train, small_world_mask=mask_sa_W_encoder, verbose=True) # Pass the SW mask here\n",
        "\n",
        "    # Step 4: Get pre-trained W and b from the autoencoder\n",
        "    W_pretrained, b_pretrained = sa.get_rvfl_params()\n",
        "\n",
        "    # Step 5: Generate direct mask for RVFL's direct connections\n",
        "    mask_direct_1d = generate_dynamic_small_world_feature_mask_fi(n_features, k, p,\n",
        "                                                                sparsity_ratio=sparsity_ratio_direct,\n",
        "                                                                feature_importances=feature_importances,\n",
        "                                                                importance_bias=importance_bias)\n",
        "\n",
        "    # Step 6: Train the RVFL with pre-trained W and b, and the sparse Bayesian solver\n",
        "    return base_rvfl(X_train, y_train, X_test, n_hidden=n_hidden, W=W_pretrained, b=b_pretrained,\n",
        "                     use_sp=True, mask_direct=mask_direct_1d, C=C, task=task,\n",
        "                     pruning_threshold=pruning_threshold)\n",
        "\n",
        "\n",
        "# --- Grid Search and Evaluation ---\n",
        "def hyperparameter_grid_search(X, y, dataset_name, task='classification'):\n",
        "    models = {\n",
        "        'Vanilla RVFL': vanilla_rvfl,\n",
        "        'SP-RVFL': sp_rvfl,\n",
        "        'SW-SP-RVFL (Dynamic)': sw_sp_rvfl,\n",
        "        'SW-SP Hidden + Direct (Dynamic)': sw_sp_hidden_direct,\n",
        "        'SA-SW-SP-RVFL (Pre-trained + Dynamic)': sa_sw_sp_rvfl # New model\n",
        "    }\n",
        "\n",
        "    neuron_options = [10, 50] # Reduced for faster execution on multiple datasets\n",
        "    C_options = [1e-2, 1.0]\n",
        "    pruning_threshold_options = [1e4] # Reduced options for faster execution\n",
        "    importance_bias_options = [0.0, 0.5] # 0.0 = no bias (random), 0.5 = moderate\n",
        "    sparsity_ratio_direct_options = [0.5] # Fixed for faster execution\n",
        "\n",
        "    # New options for Sparse Autoencoder pre-training\n",
        "    sa_lambda_l1_options = [0.001, 0.01]\n",
        "    sa_epochs_options = [20] # Reduced epochs for faster grid search, increase for better SA training\n",
        "\n",
        "    if task == 'classification':\n",
        "        X_train_full, X_test, y_train_full, y_test, encoder, y_train_raw, y_test_raw = preprocess_classification(X, y)\n",
        "        y_val_true = y_test_raw\n",
        "    else: # Regression\n",
        "        X_train_full, X_test, y_train_full, y_test_scaled, scaler_y, y_train_raw, y_test_raw = preprocess_regression(X, y)\n",
        "        y_test_original_scale = y_test_raw # The original unscaled y_test from train_test_split\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for model_name, model_fn in models.items():\n",
        "        best_score = 0 if task == 'classification' else float('inf') # For accuracy (higher is better) or RMSE (lower is better)\n",
        "        best_params = {}\n",
        "\n",
        "        params_to_try = []\n",
        "        for N in neuron_options:\n",
        "            for C in C_options:\n",
        "                current_params_base = {'N': N, 'C': C}\n",
        "\n",
        "                # Check if model uses sparse pruning (SP or SW-SP variants)\n",
        "                if 'SP' in model_name or 'SW' in model_name or 'SA' in model_name:\n",
        "                    for PT in pruning_threshold_options:\n",
        "                        current_params_sp = current_params_base.copy()\n",
        "                        current_params_sp['PT'] = PT\n",
        "\n",
        "                        # Check if model uses dynamic small-world (Dynamic variants)\n",
        "                        if 'Dynamic' in model_name or 'SA' in model_name: # SA also uses dynamic masks\n",
        "                            for IB in importance_bias_options:\n",
        "                                current_params_dynamic = current_params_sp.copy()\n",
        "                                current_params_dynamic['importance_bias'] = IB\n",
        "\n",
        "                                # Check if model uses direct connections (Hidden + Direct)\n",
        "                                if 'Direct' in model_name or 'SA-SW-SP' in model_name: # SA-SW-SP also has direct connections\n",
        "                                    for SRD in sparsity_ratio_direct_options:\n",
        "                                        final_params = current_params_dynamic.copy()\n",
        "                                        final_params['sparsity_ratio_direct'] = SRD\n",
        "\n",
        "                                        # Check if model uses Sparse Autoencoder\n",
        "                                        if 'SA' in model_name:\n",
        "                                            for L1 in sa_lambda_l1_options:\n",
        "                                                for SE in sa_epochs_options:\n",
        "                                                    sa_params = final_params.copy()\n",
        "                                                    sa_params['sa_lambda_l1'] = L1\n",
        "                                                    sa_params['sa_epochs'] = SE\n",
        "                                                    params_to_try.append(sa_params)\n",
        "                                        else: # SW-SP Hidden + Direct (Dynamic)\n",
        "                                            params_to_try.append(final_params)\n",
        "                                else: # SW-SP-RVFL (Dynamic)\n",
        "                                    params_to_try.append(current_params_dynamic)\n",
        "                        else: # For SP-RVFL (not dynamic small-world), just add PT\n",
        "                            params_to_try.append(current_params_sp)\n",
        "                else: # For Vanilla RVFL (no SP, no SW)\n",
        "                    params_to_try.append(current_params_base)\n",
        "\n",
        "        print(f\"\\n--- Starting grid search for {model_name} on {dataset_name} ({len(params_to_try)} combinations) ---\")\n",
        "        for i, params in enumerate(params_to_try):\n",
        "            N = params['N']\n",
        "            C = params['C']\n",
        "            PT = params.get('PT', 1e4) # Default if not specified (for Vanilla)\n",
        "\n",
        "            # Get dynamic specific parameters, will be None if not applicable\n",
        "            importance_bias = params.get('importance_bias', None)\n",
        "            sparsity_ratio_direct = params.get('sparsity_ratio_direct', None)\n",
        "\n",
        "            # Get SA specific parameters\n",
        "            sa_lambda_l1 = params.get('sa_lambda_l1', None)\n",
        "            sa_epochs = params.get('sa_epochs', None)\n",
        "\n",
        "            try:\n",
        "                model_kwargs = {'n_hidden': N, 'C': C, 'task': task}\n",
        "\n",
        "                if 'SP' in model_name or 'SW' in model_name or 'SA' in model_name:\n",
        "                    model_kwargs['pruning_threshold'] = PT\n",
        "\n",
        "                if 'Dynamic' in model_name or 'SA' in model_name:\n",
        "                    model_kwargs['importance_bias'] = importance_bias\n",
        "                    if 'Direct' in model_name or 'SA-SW-SP' in model_name:\n",
        "                        model_kwargs['sparsity_ratio_direct'] = sparsity_ratio_direct\n",
        "\n",
        "                if 'SA' in model_name:\n",
        "                    model_kwargs['sa_lambda_l1'] = sa_lambda_l1\n",
        "                    model_kwargs['sa_epochs'] = sa_epochs\n",
        "\n",
        "                print(f\"  Trying {i+1}/{len(params_to_try)}: Params={params}\")\n",
        "                y_val_pred = model_fn(X_train_full, y_train_full, X_test, **model_kwargs)\n",
        "\n",
        "                if task == 'classification':\n",
        "                    if y_val_pred.ndim > 1 and y_val_pred.shape[1] > 1:\n",
        "                        predicted_labels = np.argmax(y_val_pred, axis=1)\n",
        "                    else: # Binary classification\n",
        "                        predicted_labels = (y_val_pred.flatten() > 0.5).astype(int)\n",
        "\n",
        "                    acc = accuracy_score(y_val_true, predicted_labels)\n",
        "                    print(f\"    Accuracy: {acc:.4f}\")\n",
        "                    if acc > best_score:\n",
        "                        best_score = acc\n",
        "                        best_params = params\n",
        "                else: # Regression\n",
        "                    val_pred_cont = scaler_y.inverse_transform(y_val_pred)\n",
        "                    rmse = np.sqrt(mean_squared_error(y_test_original_scale, val_pred_cont))\n",
        "                    print(f\"    RMSE: {rmse:.4f}\")\n",
        "                    if rmse < best_score:\n",
        "                        best_score = rmse\n",
        "                        best_params = params\n",
        "            except Exception as e:\n",
        "                print(f\"    {model_name} failed for {params}: {e}\")\n",
        "                # import traceback\n",
        "                # traceback.print_exc() # Uncomment for full traceback during debugging\n",
        "\n",
        "        print(f\"\\nBest Hyperparameters for {model_name} on {dataset_name}:\")\n",
        "        print(best_params)\n",
        "\n",
        "        if not best_params:\n",
        "            print(f\"No successful best parameters found for {model_name} on {dataset_name}. Skipping final evaluation.\")\n",
        "            results.append({'Dataset': dataset_name, 'Model': model_name, 'Task': task, 'Best Params': {}, 'Test Score': 'N/A', 'Metric': 'N/A'})\n",
        "            continue\n",
        "\n",
        "        # Final evaluation with best hyperparameters\n",
        "        final_model_kwargs = {'n_hidden': best_params['N'], 'C': best_params['C'], 'task': task}\n",
        "\n",
        "        if 'SP' in model_name or 'SW' in model_name or 'SA' in model_name:\n",
        "            final_model_kwargs['pruning_threshold'] = best_params['PT']\n",
        "\n",
        "        if 'Dynamic' in model_name or 'SA' in model_name:\n",
        "            final_model_kwargs['importance_bias'] = best_params['importance_bias']\n",
        "            if 'Direct' in model_name or 'SA-SW-SP' in model_name:\n",
        "                final_model_kwargs['sparsity_ratio_direct'] = best_params['sparsity_ratio_direct']\n",
        "\n",
        "        if 'SA' in model_name:\n",
        "            final_model_kwargs['sa_lambda_l1'] = best_params['sa_lambda_l1']\n",
        "            final_model_kwargs['sa_epochs'] = best_params['sa_epochs']\n",
        "\n",
        "        final_pred = model_fn(X_train_full, y_train_full, X_test, **final_model_kwargs)\n",
        "\n",
        "        if task == 'classification':\n",
        "            if final_pred.ndim > 1 and final_pred.shape[1] > 1:\n",
        "                final_predicted_labels = np.argmax(final_pred, axis=1)\n",
        "            else:\n",
        "                final_predicted_labels = (final_pred.flatten() > 0.5).astype(int)\n",
        "\n",
        "            final_score = accuracy_score(y_val_true, final_predicted_labels)\n",
        "            print(f\"Test Accuracy: {final_score:.4f}\")\n",
        "            results.append({'Dataset': dataset_name, 'Model': model_name, 'Task': task, 'Best Params': best_params, 'Test Score': final_score, 'Metric': 'Accuracy'})\n",
        "\n",
        "            cm = confusion_matrix(y_val_true, final_predicted_labels)\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "            plt.title(f\"Confusion Matrix - {model_name} ({dataset_name})\")\n",
        "            plt.xlabel(\"Predicted\")\n",
        "            plt.ylabel(\"Actual\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            test_pred_cont = scaler_y.inverse_transform(final_pred)\n",
        "            final_score_rmse = np.sqrt(mean_squared_error(y_test_original_scale, test_pred_cont))\n",
        "            final_score_r2 = r2_score(y_test_original_scale, test_pred_cont)\n",
        "            print(f\"Test RMSE: {final_score_rmse:.4f}\")\n",
        "            print(f\"Test R2: {final_score_r2:.4f}\")\n",
        "            results.append({'Dataset': dataset_name, 'Model': model_name, 'Task': task, 'Best Params': best_params, 'Test RMSE': final_score_rmse, 'Test R2': final_score_r2, 'Metric': 'RMSE/R2'})\n",
        "    return results\n",
        "\n",
        "# --- Dataset Loading Functions ---\n",
        "def load_adult_dataset():\n",
        "    data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "    test_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
        "\n",
        "    if not os.path.exists('adult.data'):\n",
        "        print(\"Downloading adult.data...\")\n",
        "        urlretrieve(data_url, 'adult.data')\n",
        "    if not os.path.exists('adult.test'):\n",
        "        print(\"Downloading adult.test...\")\n",
        "        urlretrieve(test_url, 'adult.test')\n",
        "\n",
        "    column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "                    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "                    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
        "\n",
        "    df_train = pd.read_csv('adult.data', header=None, names=column_names, na_values=' ?', skipinitialspace=True)\n",
        "    # The test file has an extra row at the start and needs careful parsing for its income labels\n",
        "    df_test = pd.read_csv('adult.test', header=None, names=column_names, na_values=' ?', skipinitialspace=True, skiprows=1)\n",
        "\n",
        "    df = pd.concat([df_train, df_test], ignore_index=True)\n",
        "\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    X = df.drop('income', axis=1)\n",
        "    y = df['income']\n",
        "\n",
        "    # Clean up income labels (remove period at the end of test set labels)\n",
        "    y = y.replace({'<=50K.': '<=50K', '>50K.': '>50K'})\n",
        "\n",
        "    categorical_features = X.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_features:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "    # Convert target to binary (0 or 1)\n",
        "    y = (y == '>50K').astype(int)\n",
        "\n",
        "    return X.values.astype(np.float64), y.values.astype(np.float64)\n",
        "\n",
        "def load_mnist_dataset():\n",
        "    try:\n",
        "        from tensorflow.keras.datasets import mnist\n",
        "        print(\"Loading MNIST dataset (this may download data)...\")\n",
        "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "        X_train = X_train.reshape(-1, 28*28) / 255.0\n",
        "        X_test = X_test.reshape(-1, 28*28) / 255.0\n",
        "\n",
        "        X = np.vstack([X_train, X_test])\n",
        "        y = np.hstack([y_train, y_test])\n",
        "        return X, y\n",
        "    except ImportError:\n",
        "        print(\"TensorFlow not installed. Cannot load MNIST dataset. Skipping MNIST.\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# --- Pipeline Execution ---\n",
        "if __name__ == '__main__':\n",
        "    all_results = []\n",
        "\n",
        "    # California Housing (Regression)\n",
        "    X_california, y_california = fetch_california_housing(return_X_y=True)\n",
        "    print(\"\\n--- Running on California Housing Dataset (Regression) ---\")\n",
        "    all_results.extend(hyperparameter_grid_search(X_california, y_california, dataset_name=\"California Housing\", task='regression'))\n",
        "\n",
        "    # Adult Dataset (Classification)\n",
        "    X_adult, y_adult = load_adult_dataset()\n",
        "    print(\"\\n--- Running on Adult Dataset (Classification) ---\")\n",
        "    all_results.extend(hyperparameter_grid_search(X_adult, y_adult, dataset_name=\"Adult Income\", task='classification'))\n",
        "\n",
        "    # Diabetes Dataset (Regression)\n",
        "    X_diabetes, y_diabetes = load_diabetes(return_X_y=True)\n",
        "    print(\"\\n--- Running on Diabetes Dataset (Regression) ---\")\n",
        "    all_results.extend(hyperparameter_grid_search(X_diabetes, y_diabetes, dataset_name=\"Diabetes\", task='regression'))\n",
        "\n",
        "    # MNIST Dataset (Classification)\n",
        "    X_mnist_full, y_mnist_full = load_mnist_dataset()\n",
        "    if X_mnist_full is not None:\n",
        "        # Use a subset for faster execution during grid search\n",
        "        idx = np.random.choice(len(X_mnist_full), 5000, replace=False) # Further reduced subset size\n",
        "        X_mnist_subset = X_mnist_full[idx]\n",
        "        y_mnist_subset = y_mnist_full[idx]\n",
        "\n",
        "        print(\"\\n--- Running on MNIST Dataset (Classification - Subset) ---\")\n",
        "        all_results.extend(hyperparameter_grid_search(X_mnist_subset, y_mnist_subset, dataset_name=\"MNIST Digits (Subset)\", task='classification'))\n",
        "    else:\n",
        "        print(\"\\nSkipping MNIST dataset as TensorFlow is not installed.\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Summary of All Experiments ---\")\n",
        "    for res in all_results:\n",
        "        print(res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5LyhRUKz2p5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xwIS2SAvdwF",
        "outputId": "aa51e3cc-9121-445d-8a9a-e4a10a1be446"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading Dataset: letter ---\n",
            "Data loaded. Task: classification. Train shape: (12800, 16)\n",
            "\n",
            "--- Grid Search: vanilla_rvfl ---\n",
            "Testing: {'n_hidden': 64, 'C': 10}\n",
            " -> Validation Accuracy: 0.7259\n",
            "Testing: {'n_hidden': 64, 'C': 1}\n",
            " -> Validation Accuracy: 0.7281\n",
            "Testing: {'n_hidden': 64, 'C': 0.1}\n",
            " -> Validation Accuracy: 0.7284\n",
            "Testing: {'n_hidden': 64, 'C': 0.01}\n",
            " -> Validation Accuracy: 0.7228\n",
            "Testing: {'n_hidden': 64, 'C': 0.001}\n",
            " -> Validation Accuracy: 0.7081\n",
            "Testing: {'n_hidden': 128, 'C': 10}\n",
            " -> Validation Accuracy: 0.7731\n",
            "Testing: {'n_hidden': 128, 'C': 1}\n",
            " -> Validation Accuracy: 0.7884\n",
            "Testing: {'n_hidden': 128, 'C': 0.1}\n",
            " -> Validation Accuracy: 0.7753\n",
            "Testing: {'n_hidden': 128, 'C': 0.01}\n",
            " -> Validation Accuracy: 0.7641\n",
            "Testing: {'n_hidden': 128, 'C': 0.001}\n",
            " -> Validation Accuracy: 0.7791\n",
            "Testing: {'n_hidden': 256, 'C': 10}\n",
            " -> Validation Accuracy: 0.8337\n",
            "Testing: {'n_hidden': 256, 'C': 1}\n",
            " -> Validation Accuracy: 0.8444\n",
            "Testing: {'n_hidden': 256, 'C': 0.1}\n",
            " -> Validation Accuracy: 0.8413\n",
            "Testing: {'n_hidden': 256, 'C': 0.01}\n",
            " -> Validation Accuracy: 0.8378\n",
            "Testing: {'n_hidden': 256, 'C': 0.001}\n",
            " -> Validation Accuracy: 0.8441\n",
            "Testing: {'n_hidden': 512, 'C': 10}\n",
            " -> Validation Accuracy: 0.8919\n",
            "Testing: {'n_hidden': 512, 'C': 1}\n",
            " -> Validation Accuracy: 0.8853\n",
            "Testing: {'n_hidden': 512, 'C': 0.1}\n",
            " -> Validation Accuracy: 0.8859\n",
            "Testing: {'n_hidden': 512, 'C': 0.01}\n",
            " -> Validation Accuracy: 0.8884\n",
            "Testing: {'n_hidden': 512, 'C': 0.001}\n",
            " -> Validation Accuracy: 0.8881\n",
            "Testing: {'n_hidden': 1024, 'C': 10}\n",
            " -> Validation Accuracy: 0.9294\n",
            "Testing: {'n_hidden': 1024, 'C': 1}\n",
            " -> Validation Accuracy: 0.9309\n",
            "Testing: {'n_hidden': 1024, 'C': 0.1}\n",
            " -> Validation Accuracy: 0.9272\n",
            "Testing: {'n_hidden': 1024, 'C': 0.01}\n",
            " -> Validation Accuracy: 0.9303\n",
            "Testing: {'n_hidden': 1024, 'C': 0.001}\n",
            " -> Validation Accuracy: 0.9244\n",
            "\n",
            "Best Params: {'n_hidden': 1024, 'C': 1}\n",
            "Best Validation Accuracy: 0.9309\n",
            "Grid Search Time: 11.43s\n",
            "\n",
            "--- Final Training on Full Data ---\n",
            "FINAL TEST ACCURACY: 0.9303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.datasets import fetch_openml, fetch_california_housing\n",
        "import time\n",
        "from itertools import product\n",
        "\n",
        "# --- Correntropy Loss Gradient ---\n",
        "def correntropy_loss_grad(y_true, y_pred, sigma=1.0):\n",
        "    error = y_pred - y_true\n",
        "    return (error / (sigma ** 2)) * np.exp(- (error ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "# --- Vanilla RVFL with Closed-form Ridge Regression ---\n",
        "def vanilla_rvfl(X_train, y_train, X_val, n_hidden=256, C=1e-3):\n",
        "    n_features = X_train.shape[1]\n",
        "    n_outputs = y_train.shape[1]\n",
        "\n",
        "    W = np.random.randn(n_features, n_hidden)\n",
        "    b = np.random.randn(n_hidden)\n",
        "\n",
        "    def activation(x): return np.maximum(0, x)\n",
        "    H_train = activation(X_train @ W + b)\n",
        "    H_val = activation(X_val @ W + b)\n",
        "\n",
        "    Z_train = np.hstack([X_train, H_train])\n",
        "    Z_val = np.hstack([X_val, H_val])\n",
        "\n",
        "    I = np.eye(Z_train.shape[1])\n",
        "    beta = np.linalg.solve(Z_train.T @ Z_train + C * I, Z_train.T @ y_train)\n",
        "\n",
        "    return Z_val @ beta, beta\n",
        "\n",
        "# --- Closed-form Correntropy-based RVFL ---\n",
        "def corrrvfl_closed_form(X_train, y_train, X_val, n_hidden=256, sigma=1.0, C=1e-3, max_iter=50, tol=1e-5):\n",
        "    n_features = X_train.shape[1]\n",
        "    n_outputs = y_train.shape[1]\n",
        "\n",
        "    W = np.random.randn(n_features, n_hidden)\n",
        "    b = np.random.randn(n_hidden)\n",
        "\n",
        "    def activation(x): return np.maximum(0, x)\n",
        "    H_train = activation(X_train @ W + b)\n",
        "    H_val = activation(X_val @ W + b)\n",
        "\n",
        "    Z_train = np.hstack([X_train, H_train])\n",
        "    Z_val = np.hstack([X_val, H_val])\n",
        "\n",
        "    beta = np.random.randn(Z_train.shape[1], n_outputs) * 0.01\n",
        "\n",
        "    for t in range(max_iter):\n",
        "        y_pred = Z_train @ beta\n",
        "        residuals = y_train - y_pred\n",
        "        weights = np.exp(- (residuals ** 2) / (2 * sigma ** 2))\n",
        "        W_diag = np.mean(weights, axis=1)\n",
        "        W_matrix = np.diag(W_diag)\n",
        "\n",
        "        beta_new = np.linalg.solve(Z_train.T @ W_matrix @ Z_train + C * np.eye(Z_train.shape[1]),\n",
        "                                   Z_train.T @ W_matrix @ y_train)\n",
        "\n",
        "        if np.linalg.norm(beta_new - beta) < tol:\n",
        "            break\n",
        "        beta = beta_new\n",
        "\n",
        "    return Z_val @ beta, beta\n",
        "\n",
        "# --- Data Loader ---\n",
        "def load_dataset(name='mnist'):\n",
        "    print(f\"\\n--- Loading Dataset: {name} ---\")\n",
        "    if name == 'mnist':\n",
        "        X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "        X = X / 255.0\n",
        "        y = y.astype(int)\n",
        "        y_processed = LabelBinarizer().fit_transform(y)\n",
        "        X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "            X, y_processed, test_size=10000, stratify=y)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_full, y_train_full, test_size=0.2, stratify=np.argmax(y_train_full, axis=1))\n",
        "        scaler_X = StandardScaler().fit(X_train)\n",
        "        scaler_y = None\n",
        "        task_type = 'classification'\n",
        "    elif name == 'california_housing':\n",
        "        X, y = fetch_california_housing(return_X_y=True)\n",
        "        y = y.reshape(-1, 1)\n",
        "        X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
        "        scaler_X = StandardScaler().fit(X_train)\n",
        "        scaler_y = StandardScaler().fit(y_train)\n",
        "        y_train = scaler_y.transform(y_train)\n",
        "        y_val = scaler_y.transform(y_val)\n",
        "        y_test = scaler_y.transform(y_test)\n",
        "        task_type = 'regression'\n",
        "    elif name == 'letter':\n",
        "        X, y = fetch_openml(name='letter', version=1, return_X_y=True, as_frame=False)\n",
        "        y = LabelBinarizer().fit_transform(y)\n",
        "        X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, stratify=y.argmax(axis=1), random_state=42)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_full, y_train_full, test_size=0.2, stratify=y_train_full.argmax(axis=1), random_state=42)\n",
        "        scaler_X = StandardScaler().fit(X_train)\n",
        "        scaler_y = None\n",
        "        task_type = 'classification'\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset '{name}' not supported.\")\n",
        "\n",
        "    X_train = scaler_X.transform(X_train)\n",
        "    X_val = scaler_X.transform(X_val)\n",
        "    X_test = scaler_X.transform(X_test)\n",
        "\n",
        "    print(f\"Data loaded. Task: {task_type}. Train shape: {X_train.shape}\")\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler_X, scaler_y, task_type\n",
        "\n",
        "# --- Evaluation Pipeline ---\n",
        "def run_evaluation(dataset_name, model_type='corrrvfl_closed_form'):\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, scaler_X, scaler_y, task_type = load_dataset(dataset_name)\n",
        "\n",
        "    if model_type == 'corrrvfl_closed_form':\n",
        "        param_grid = {\n",
        "            'n_hidden': [128, 256, 512, 1024],\n",
        "            'C': [1e-3, 1e-2, 1e-1],\n",
        "            'sigma': [0.5, 1.0]\n",
        "        }\n",
        "    else:\n",
        "        param_grid = {\n",
        "            'n_hidden': [64, 128, 256, 512, 1024],\n",
        "            'C': [10, 1, 1e-1, 1e-2, 1e-3]\n",
        "        }\n",
        "\n",
        "    keys = list(param_grid.keys())\n",
        "    best_score = -1 if task_type == 'classification' else float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    print(f\"\\n--- Grid Search: {model_type} ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for combo in product(*param_grid.values()):\n",
        "        params = dict(zip(keys, combo))\n",
        "        print(f\"Testing: {params}\")\n",
        "\n",
        "        if model_type == 'corrrvfl_closed_form':\n",
        "            y_val_pred, _ = corrrvfl_closed_form(X_train, y_train, X_val, **params)\n",
        "        else:\n",
        "            y_val_pred, _ = vanilla_rvfl(X_train, y_train, X_val, **params)\n",
        "\n",
        "        if task_type == 'classification':\n",
        "            y_pred_labels = np.argmax(y_val_pred, axis=1)\n",
        "            y_true_labels = np.argmax(y_val, axis=1)\n",
        "            score = accuracy_score(y_true_labels, y_pred_labels)\n",
        "            print(f\" -> Validation Accuracy: {score:.4f}\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = params\n",
        "        else:\n",
        "            y_val_pred_rescaled = scaler_y.inverse_transform(y_val_pred)\n",
        "            y_val_true_rescaled = scaler_y.inverse_transform(y_val)\n",
        "            score = np.sqrt(mean_squared_error(y_val_true_rescaled, y_val_pred_rescaled))\n",
        "            print(f\" -> Validation RMSE: {score:.4f}\")\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_params = params\n",
        "\n",
        "    print(f\"\\nBest Params: {best_params}\")\n",
        "    print(f\"Best Validation {'Accuracy' if task_type == 'classification' else 'RMSE'}: {best_score:.4f}\")\n",
        "    print(f\"Grid Search Time: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    X_train_full = np.vstack([X_train, X_val])\n",
        "    y_train_full = np.vstack([y_train, y_val])\n",
        "\n",
        "    print(\"\\n--- Final Training on Full Data ---\")\n",
        "    if model_type == 'corrrvfl_closed_form':\n",
        "        y_test_pred, _ = corrrvfl_closed_form(X_train_full, y_train_full, X_test, **best_params)\n",
        "    else:\n",
        "        y_test_pred, _ = vanilla_rvfl(X_train_full, y_train_full, X_test, **best_params)\n",
        "\n",
        "    if task_type == 'classification':\n",
        "        test_pred = np.argmax(y_test_pred, axis=1)\n",
        "        test_true = np.argmax(y_test, axis=1)\n",
        "        print(f\"FINAL TEST ACCURACY: {accuracy_score(test_true, test_pred):.4f}\")\n",
        "    else:\n",
        "        y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
        "        y_test_true_rescaled = scaler_y.inverse_transform(y_test)\n",
        "        print(f\"FINAL TEST RMSE: {np.sqrt(mean_squared_error(y_test_true_rescaled, y_test_pred_rescaled)):.4f}\")\n",
        "\n",
        "# --- Main ---\n",
        "if __name__ == '__main__':\n",
        "    MODEL_TO_RUN = 'corrrvfl_closed_form'  # 'vanilla_rvfl' or 'corrrvfl_closed_form'\n",
        "    DATASET_TO_RUN = 'letter'  # or 'california_housing'\n",
        "    run_evaluation(DATASET_TO_RUN, MODEL_TO_RUN)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyIuHZbz-vkV",
        "outputId": "183896c9-0bbe-433f-a4ce-82c8d60c77f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading Dataset: letter ---\n",
            "Data loaded. Task: classification. Train shape: (12800, 16)\n",
            "\n",
            "--- Grid Search: corrrvfl_closed_form ---\n",
            "Testing: {'n_hidden': 128, 'C': 0.001, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.7775\n",
            "Testing: {'n_hidden': 128, 'C': 0.001, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.7837\n",
            "Testing: {'n_hidden': 128, 'C': 0.01, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.7709\n",
            "Testing: {'n_hidden': 128, 'C': 0.01, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.7781\n",
            "Testing: {'n_hidden': 128, 'C': 0.1, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.7688\n",
            "Testing: {'n_hidden': 128, 'C': 0.1, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.7684\n",
            "Testing: {'n_hidden': 256, 'C': 0.001, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.8438\n",
            "Testing: {'n_hidden': 256, 'C': 0.001, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.8450\n",
            "Testing: {'n_hidden': 256, 'C': 0.01, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.8363\n",
            "Testing: {'n_hidden': 256, 'C': 0.01, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.8344\n",
            "Testing: {'n_hidden': 256, 'C': 0.1, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.8391\n",
            "Testing: {'n_hidden': 256, 'C': 0.1, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.8344\n",
            "Testing: {'n_hidden': 512, 'C': 0.001, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.8944\n",
            "Testing: {'n_hidden': 512, 'C': 0.001, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.8891\n",
            "Testing: {'n_hidden': 512, 'C': 0.01, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.8900\n",
            "Testing: {'n_hidden': 512, 'C': 0.01, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.8916\n",
            "Testing: {'n_hidden': 512, 'C': 0.1, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.8897\n",
            "Testing: {'n_hidden': 512, 'C': 0.1, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.8866\n",
            "Testing: {'n_hidden': 1024, 'C': 0.001, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.9325\n",
            "Testing: {'n_hidden': 1024, 'C': 0.001, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.9297\n",
            "Testing: {'n_hidden': 1024, 'C': 0.01, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.9313\n",
            "Testing: {'n_hidden': 1024, 'C': 0.01, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.9291\n",
            "Testing: {'n_hidden': 1024, 'C': 0.1, 'sigma': 0.5}\n",
            " -> Validation Accuracy: 0.9272\n",
            "Testing: {'n_hidden': 1024, 'C': 0.1, 'sigma': 1.0}\n",
            " -> Validation Accuracy: 0.9300\n",
            "\n",
            "Best Params: {'n_hidden': 1024, 'C': 0.001, 'sigma': 0.5}\n",
            "Best Validation Accuracy: 0.9325\n",
            "Grid Search Time: 1248.40s\n",
            "\n",
            "--- Final Training on Full Data ---\n",
            "FINAL TEST ACCURACY: 0.9293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.datasets import make_regression # For synthetic data\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "class SWSPRVFL:\n",
        "    \"\"\"\n",
        "    Small-World Sparse Random Vector Functional Link (SW-SP-RVFL) Network.\n",
        "\n",
        "    This class implements an RVFL network where:\n",
        "    1. The hidden layer weights and biases are intelligently initialized\n",
        "       using a pre-trained Sparse Autoencoder (SAE).\n",
        "    2. The connections from the input to the RVFL hidden layer are structured\n",
        "       to exhibit small-world properties (simplified Watts-Strogatz model).\n",
        "    3. The output layer weights are learned via a closed-form solution (pseudo-inverse).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 hidden_rvfl_dim: int = 128,\n",
        "                 sparsity_target: float = 0.05,\n",
        "                 sparsity_weight: float = 0.01,\n",
        "                 sae_epochs: int = 50,\n",
        "                 sae_batch_size: int = 64,\n",
        "                 sae_learning_rate: float = 0.001,\n",
        "                 rvfl_ridge_lambda: float = 1e-3,\n",
        "                 small_world_k_neighbors: int = 5,\n",
        "                 small_world_p_rewire: float = 0.1):\n",
        "        \"\"\"\n",
        "        Initializes the SWSPRVFL model.\n",
        "\n",
        "        Args:\n",
        "            hidden_rvfl_dim (int): Number of neurons in the RVFL's hidden layer.\n",
        "                                   This also defines the SAE's hidden layer size.\n",
        "            sparsity_target (float): Desired average activation of SAE hidden neurons.\n",
        "                                     Used for KL divergence penalty if implemented,\n",
        "                                     or conceptually for L1 regularization.\n",
        "            sparsity_weight (float): Weight for the sparsity penalty in the SAE loss function.\n",
        "            sae_epochs (int): Number of epochs to train the Sparse Autoencoder.\n",
        "            sae_batch_size (int): Batch size for SAE training.\n",
        "            sae_learning_rate (float): Learning rate for the SAE optimizer.\n",
        "            rvfl_ridge_lambda (float): Regularization parameter for the RVFL output layer\n",
        "                                       (Ridge Regression equivalent for pseudo-inverse).\n",
        "            small_world_k_neighbors (int): Initial number of 'local' connections for\n",
        "                                           each RVFL hidden neuron in the small-world model.\n",
        "            small_world_p_rewire (float): Probability of rewiring a 'local' connection\n",
        "                                          to a random input feature, creating shortcuts.\n",
        "        \"\"\"\n",
        "        self.hidden_rvfl_dim = hidden_rvfl_dim\n",
        "        self.sparsity_target = sparsity_target\n",
        "        self.sparsity_weight = sparsity_weight\n",
        "        self.sae_epochs = sae_epochs\n",
        "        self.sae_batch_size = sae_batch_size\n",
        "        self.sae_learning_rate = sae_learning_rate\n",
        "        self.rvfl_ridge_lambda = rvfl_ridge_lambda\n",
        "        self.small_world_k_neighbors = small_world_k_neighbors\n",
        "        self.small_world_p_rewire = small_world_p_rewire\n",
        "\n",
        "        # Model parameters to be learned/derived\n",
        "        self.W_enc = None  # Encoder weights from SAE\n",
        "        self.b_enc = None  # Encoder biases from SAE\n",
        "        self.W_rvfl_effective = None # Effective input-to-hidden weights for RVFL (small-world structured)\n",
        "        self.b_rvfl_effective = None # Effective hidden biases for RVFL (from SAE)\n",
        "        self.W_out = None  # Output layer weights of RVFL\n",
        "\n",
        "        # Scalers for input and output data\n",
        "        self.scaler_X = StandardScaler()\n",
        "        self.scaler_Y = StandardScaler() # For regression targets\n",
        "\n",
        "    def _build_and_train_sae(self, X_train_scaled: np.ndarray):\n",
        "        \"\"\"\n",
        "        Builds and trains the Sparse Autoencoder.\n",
        "\n",
        "        Args:\n",
        "            X_train_scaled (np.ndarray): Scaled training input data.\n",
        "        \"\"\"\n",
        "        input_dim = X_train_scaled.shape[1]\n",
        "\n",
        "        # SAE Encoder\n",
        "        encoder_input = Input(shape=(input_dim,))\n",
        "        # Using L1 regularization to encourage sparsity in the hidden layer activations\n",
        "        encoded = Dense(self.hidden_rvfl_dim,\n",
        "                        activation='sigmoid', # Sigmoid is often used with sparsity penalties\n",
        "                        activity_regularizer=l1(self.sparsity_weight),\n",
        "                        name='encoder_output')(encoder_input)\n",
        "\n",
        "        # SAE Decoder\n",
        "        decoded = Dense(input_dim, activation='sigmoid')(encoded) # Sigmoid for reconstruction to [0,1] range\n",
        "\n",
        "        # Autoencoder model\n",
        "        autoencoder = Model(inputs=encoder_input, outputs=decoded)\n",
        "        autoencoder.compile(optimizer=Adam(learning_rate=self.sae_learning_rate), loss='mse')\n",
        "\n",
        "        print(\"\\n--- Training Sparse Autoencoder ---\")\n",
        "        autoencoder.fit(X_train_scaled, X_train_scaled,\n",
        "                        epochs=self.sae_epochs,\n",
        "                        batch_size=self.sae_batch_size,\n",
        "                        shuffle=True,\n",
        "                        verbose=1,\n",
        "                        validation_split=0.1) # Use a validation split for monitoring\n",
        "\n",
        "        # Extract weights and biases from the encoder layer\n",
        "        encoder_layer = autoencoder.get_layer('encoder_output')\n",
        "        self.W_enc, self.b_enc = encoder_layer.get_weights()\n",
        "        print(f\"SAE Encoder Weights (W_enc) shape: {self.W_enc.shape}\")\n",
        "        print(f\"SAE Encoder Biases (b_enc) shape: {self.b_enc.shape}\")\n",
        "\n",
        "    def _build_small_world_rvfl_hidden_weights(self, input_dim: int):\n",
        "        \"\"\"\n",
        "        Constructs the effective input-to-hidden weight matrix for the RVFL\n",
        "        based on SAE weights and a simplified small-world connectivity.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Dimension of the input features.\n",
        "        \"\"\"\n",
        "        self.W_rvfl_effective = np.zeros((input_dim, self.hidden_rvfl_dim))\n",
        "        self.b_rvfl_effective = self.b_enc # RVFL hidden biases are directly from SAE biases\n",
        "\n",
        "        print(\"\\n--- Applying Small-World Connectivity to RVFL Hidden Layer ---\")\n",
        "        for i in range(self.hidden_rvfl_dim): # Iterate through each RVFL hidden neuron\n",
        "            # Identify input features with the strongest connections from SAE for this hidden neuron\n",
        "            # This simulates the 'local' connections or initial lattice structure\n",
        "            # np.argsort returns indices that would sort the array. [::-1] reverses to get descending order.\n",
        "            # [:self.small_world_k_neighbors] selects the top K.\n",
        "            top_k_indices = np.argsort(np.abs(self.W_enc[:, i]))[::-1][:self.small_world_k_neighbors]\n",
        "\n",
        "            for j_original in top_k_indices:\n",
        "                if np.random.rand() < self.small_world_p_rewire:\n",
        "                    # Rewire: Select a random input feature index for a 'shortcut'\n",
        "                    j_rewired = np.random.choice(input_dim)\n",
        "                    # Assign the SAE-learned weight for this rewired connection\n",
        "                    self.W_rvfl_effective[j_rewired, i] = self.W_enc[j_rewired, i]\n",
        "                    # print(f\"  Rewired connection for hidden node {i} from input {j_original} to {j_rewired}\")\n",
        "                else:\n",
        "                    # Keep the original 'local' connection\n",
        "                    self.W_rvfl_effective[j_original, i] = self.W_enc[j_original, i]\n",
        "                    # print(f\"  Kept local connection for hidden node {i} from input {j_original}\")\n",
        "\n",
        "        # Calculate sparsity of the effective weight matrix\n",
        "        sparsity_percentage = np.count_nonzero(self.W_rvfl_effective) / \\\n",
        "                              (input_dim * self.hidden_rvfl_dim) * 100\n",
        "        print(f\"Sparsity of RVFL effective hidden weights: {sparsity_percentage:.2f}%\")\n",
        "        print(f\"RVFL Effective Hidden Weights (W_rvfl_effective) shape: {self.W_rvfl_effective.shape}\")\n",
        "        print(f\"RVFL Effective Hidden Biases (b_rvfl_effective) shape: {self.b_rvfl_effective.shape}\")\n",
        "\n",
        "\n",
        "    def _calculate_rvfl_hidden_activations(self, X_data_scaled: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculates the activations of the RVFL hidden layer using the\n",
        "        small-world structured weights and biases.\n",
        "\n",
        "        Args:\n",
        "            X_data_scaled (np.ndarray): Scaled input data.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Activations of the RVFL hidden layer.\n",
        "        \"\"\"\n",
        "        # The activation function for the RVFL hidden layer (consistent with SAE encoder)\n",
        "        # Using sigmoid here, but can be changed (e.g., np.tanh, or custom ReLU implementation)\n",
        "        return 1 / (1 + np.exp(-(X_data_scaled @ self.W_rvfl_effective + self.b_rvfl_effective)))\n",
        "\n",
        "    def train(self, X: np.ndarray, Y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Trains the SW-SP-RVFL model.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Training input features.\n",
        "            Y (np.ndarray): Training target outputs.\n",
        "        \"\"\"\n",
        "        if X.shape[0] != Y.shape[0]:\n",
        "            raise ValueError(\"Number of samples in X and Y must be the same.\")\n",
        "\n",
        "        input_dim = X.shape[1]\n",
        "        output_dim = Y.shape[1] if Y.ndim > 1 else 1 # Handle single output regression\n",
        "\n",
        "        # 1. Data Scaling\n",
        "        X_scaled = self.scaler_X.fit_transform(X)\n",
        "        Y_scaled = self.scaler_Y.fit_transform(Y.reshape(-1, 1)).flatten() if Y.ndim == 1 else self.scaler_Y.fit_transform(Y)\n",
        "\n",
        "        # 2. Phase 1: Sparse Autoencoder Training\n",
        "        self._build_and_train_sae(X_scaled)\n",
        "\n",
        "        # 3. Phase 2: RVFL Hidden Layer Construction with Small-World Structure\n",
        "        self._build_small_world_rvfl_hidden_weights(input_dim)\n",
        "\n",
        "        # 4. Phase 3: RVFL Output Layer Learning (Linear Least Squares)\n",
        "        print(\"\\n--- Training RVFL Output Layer ---\")\n",
        "        # Calculate hidden layer outputs for the training data\n",
        "        H_train_sw = self._calculate_rvfl_hidden_activations(X_scaled)\n",
        "\n",
        "        # Concatenate original inputs with the hidden layer outputs\n",
        "        # This forms the augmented feature matrix for the output layer\n",
        "        Phi_train = np.hstack((X_scaled, H_train_sw))\n",
        "        print(f\"Augmented features (Phi_train) shape: {Phi_train.shape}\")\n",
        "\n",
        "        # Calculate the output weights (W_out) using Moore-Penrose pseudo-inverse\n",
        "        # Add a small regularization term (Ridge Regression equivalent) for numerical stability\n",
        "        identity_matrix = np.eye(Phi_train.shape[1])\n",
        "        try:\n",
        "            # (Phi_train.T @ Phi_train + lambda * I)^-1 @ Phi_train.T @ Y_train\n",
        "            self.W_out = np.linalg.pinv(Phi_train.T @ Phi_train + self.rvfl_ridge_lambda * identity_matrix) @ Phi_train.T @ Y_scaled\n",
        "        except np.linalg.LinAlgError as e:\n",
        "            print(f\"Linear algebra error during pseudo-inverse calculation: {e}\")\n",
        "            print(\"Consider increasing rvfl_ridge_lambda or checking data for singularity.\")\n",
        "            raise\n",
        "\n",
        "        print(f\"RVFL Output Weights (W_out) shape: {self.W_out.shape}\")\n",
        "        print(\"Model training complete.\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Makes predictions using the trained SW-SP-RVFL model.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input features for prediction.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted outputs.\n",
        "        \"\"\"\n",
        "        if self.W_out is None:\n",
        "            raise RuntimeError(\"Model has not been trained. Call .train() first.\")\n",
        "\n",
        "        # Scale the input data\n",
        "        X_scaled = self.scaler_X.transform(X)\n",
        "\n",
        "        # Calculate hidden layer outputs using the fixed small-world structured weights\n",
        "        H_sw = self._calculate_rvfl_hidden_activations(X_scaled)\n",
        "\n",
        "        # Concatenate original inputs with hidden layer outputs\n",
        "        Phi = np.hstack((X_scaled, H_sw))\n",
        "\n",
        "        # Make predictions\n",
        "        Y_pred_scaled = Phi @ self.W_out\n",
        "\n",
        "        # Inverse transform the predictions to the original scale\n",
        "        Y_pred = self.scaler_Y.inverse_transform(Y_pred_scaled.reshape(-1, 1))\n",
        "        return Y_pred.flatten() if Y_pred.shape[1] == 1 else Y_pred\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Generating Synthetic Data ---\")\n",
        "    # Generate a synthetic regression dataset\n",
        "    # n_samples: number of data points\n",
        "    # n_features: number of input features\n",
        "    # n_targets: number of output targets\n",
        "    # noise: standard deviation of Gaussian noise added to the output\n",
        "    X, Y = make_regression(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_targets=1, noise=10.0, random_state=42)\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(f\"X_train shape: {X_train.shape}\")\n",
        "    print(f\"Y_train shape: {Y_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}\")\n",
        "    print(f\"Y_test shape: {Y_test.shape}\")\n",
        "\n",
        "    # Initialize and train the SW-SP-RVFL model\n",
        "    model = SWSPRVFL(\n",
        "        hidden_rvfl_dim=100,          # Number of hidden nodes\n",
        "        sparsity_target=0.05,         # Desired sparsity for SAE\n",
        "        sparsity_weight=0.001,        # L1 regularization weight for SAE\n",
        "        sae_epochs=200,               # SAE training epochs\n",
        "        sae_batch_size=32,\n",
        "        sae_learning_rate=0.005,\n",
        "        rvfl_ridge_lambda=1e-6,       # Regularization for RVFL output layer\n",
        "        small_world_k_neighbors=3,    # Initial local connections for small-world\n",
        "        small_world_p_rewire=0.15     # Rewiring probability for small-world\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        model.train(X_train, Y_train)\n",
        "\n",
        "        # Make predictions on the test set\n",
        "        Y_pred = model.predict(X_test)\n",
        "\n",
        "        # Evaluate the model\n",
        "        mse = mean_squared_error(Y_test, Y_pred)\n",
        "        r2 = r2_score(Y_test, Y_pred)\n",
        "\n",
        "        print(\"\\n--- Model Evaluation ---\")\n",
        "        print(f\"Test Mean Squared Error (MSE): {mse:.4f}\")\n",
        "        print(f\"Test R-squared (R2): {r2:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during model training or prediction: {e}\")\n",
        "        print(\"Please review the error message and ensure all dependencies are installed.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJuMb7lIDPa_",
        "outputId": "a55c2b7b-ebf8-4f2d-bb3f-21dd3d87db40"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating Synthetic Data ---\n",
            "X_train shape: (800, 20)\n",
            "Y_train shape: (800,)\n",
            "X_test shape: (200, 20)\n",
            "Y_test shape: (200,)\n",
            "\n",
            "--- Training Sparse Autoencoder ---\n",
            "Epoch 1/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 2.6837 - val_loss: 2.4245\n",
            "Epoch 2/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2.4492 - val_loss: 2.3494\n",
            "Epoch 3/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.3469 - val_loss: 2.2621\n",
            "Epoch 4/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2.2362 - val_loss: 2.1545\n",
            "Epoch 5/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2.1243 - val_loss: 2.0494\n",
            "Epoch 6/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.0181 - val_loss: 1.9513\n",
            "Epoch 7/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.9216 - val_loss: 1.8630\n",
            "Epoch 8/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.8350 - val_loss: 1.7845\n",
            "Epoch 9/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.7574 - val_loss: 1.7140\n",
            "Epoch 10/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.6871 - val_loss: 1.6502\n",
            "Epoch 11/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6231 - val_loss: 1.5920\n",
            "Epoch 12/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.5645 - val_loss: 1.5387\n",
            "Epoch 13/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5106 - val_loss: 1.4898\n",
            "Epoch 14/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.4611 - val_loss: 1.4449\n",
            "Epoch 15/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.4155 - val_loss: 1.4036\n",
            "Epoch 16/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1.3736 - val_loss: 1.3658\n",
            "Epoch 17/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 1.3351 - val_loss: 1.3311\n",
            "Epoch 18/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.2997 - val_loss: 1.2994\n",
            "Epoch 19/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2673 - val_loss: 1.2703\n",
            "Epoch 20/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.2374 - val_loss: 1.2436\n",
            "Epoch 21/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.2099 - val_loss: 1.2191\n",
            "Epoch 22/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.1846 - val_loss: 1.1965\n",
            "Epoch 23/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1612 - val_loss: 1.1757\n",
            "Epoch 24/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.1396 - val_loss: 1.1564\n",
            "Epoch 25/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1196 - val_loss: 1.1387\n",
            "Epoch 26/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.1010 - val_loss: 1.1221\n",
            "Epoch 27/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.0837 - val_loss: 1.1068\n",
            "Epoch 28/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.0676 - val_loss: 1.0925\n",
            "Epoch 29/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.0526 - val_loss: 1.0792\n",
            "Epoch 30/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.0385 - val_loss: 1.0667\n",
            "Epoch 31/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.0254 - val_loss: 1.0551\n",
            "Epoch 32/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.0131 - val_loss: 1.0441\n",
            "Epoch 33/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.0015 - val_loss: 1.0338\n",
            "Epoch 34/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9906 - val_loss: 1.0241\n",
            "Epoch 35/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9803 - val_loss: 1.0150\n",
            "Epoch 36/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9706 - val_loss: 1.0064\n",
            "Epoch 37/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.9614 - val_loss: 0.9982\n",
            "Epoch 38/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9527 - val_loss: 0.9905\n",
            "Epoch 39/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.9444 - val_loss: 0.9831\n",
            "Epoch 40/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9366 - val_loss: 0.9762\n",
            "Epoch 41/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.9292 - val_loss: 0.9696\n",
            "Epoch 42/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.9221 - val_loss: 0.9632\n",
            "Epoch 43/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.9153 - val_loss: 0.9572\n",
            "Epoch 44/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.9089 - val_loss: 0.9515\n",
            "Epoch 45/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9027 - val_loss: 0.9460\n",
            "Epoch 46/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.8968 - val_loss: 0.9407\n",
            "Epoch 47/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.8912 - val_loss: 0.9357\n",
            "Epoch 48/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8858 - val_loss: 0.9309\n",
            "Epoch 49/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8806 - val_loss: 0.9262\n",
            "Epoch 50/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8757 - val_loss: 0.9218\n",
            "Epoch 51/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8709 - val_loss: 0.9175\n",
            "Epoch 52/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.8663 - val_loss: 0.9134\n",
            "Epoch 53/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.8619 - val_loss: 0.9095\n",
            "Epoch 54/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.8576 - val_loss: 0.9057\n",
            "Epoch 55/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.8535 - val_loss: 0.9020\n",
            "Epoch 56/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8495 - val_loss: 0.8985\n",
            "Epoch 57/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8457 - val_loss: 0.8951\n",
            "Epoch 58/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8420 - val_loss: 0.8918\n",
            "Epoch 59/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8384 - val_loss: 0.8886\n",
            "Epoch 60/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8350 - val_loss: 0.8855\n",
            "Epoch 61/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8316 - val_loss: 0.8825\n",
            "Epoch 62/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8284 - val_loss: 0.8796\n",
            "Epoch 63/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8253 - val_loss: 0.8769\n",
            "Epoch 64/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8222 - val_loss: 0.8741\n",
            "Epoch 65/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8193 - val_loss: 0.8715\n",
            "Epoch 66/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8164 - val_loss: 0.8690\n",
            "Epoch 67/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8136 - val_loss: 0.8665\n",
            "Epoch 68/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8109 - val_loss: 0.8641\n",
            "Epoch 69/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8083 - val_loss: 0.8618\n",
            "Epoch 70/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8057 - val_loss: 0.8596\n",
            "Epoch 71/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8032 - val_loss: 0.8574\n",
            "Epoch 72/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8008 - val_loss: 0.8553\n",
            "Epoch 73/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7985 - val_loss: 0.8532\n",
            "Epoch 74/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7962 - val_loss: 0.8512\n",
            "Epoch 75/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7940 - val_loss: 0.8492\n",
            "Epoch 76/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7918 - val_loss: 0.8473\n",
            "Epoch 77/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7897 - val_loss: 0.8455\n",
            "Epoch 78/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7876 - val_loss: 0.8437\n",
            "Epoch 79/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7856 - val_loss: 0.8419\n",
            "Epoch 80/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7836 - val_loss: 0.8402\n",
            "Epoch 81/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7817 - val_loss: 0.8386\n",
            "Epoch 82/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7798 - val_loss: 0.8369\n",
            "Epoch 83/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7779 - val_loss: 0.8354\n",
            "Epoch 84/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7761 - val_loss: 0.8338\n",
            "Epoch 85/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7744 - val_loss: 0.8323\n",
            "Epoch 86/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7727 - val_loss: 0.8309\n",
            "Epoch 87/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7710 - val_loss: 0.8294\n",
            "Epoch 88/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7693 - val_loss: 0.8280\n",
            "Epoch 89/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7677 - val_loss: 0.8267\n",
            "Epoch 90/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7661 - val_loss: 0.8254\n",
            "Epoch 91/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7646 - val_loss: 0.8241\n",
            "Epoch 92/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7631 - val_loss: 0.8228\n",
            "Epoch 93/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7616 - val_loss: 0.8216\n",
            "Epoch 94/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7601 - val_loss: 0.8204\n",
            "Epoch 95/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7587 - val_loss: 0.8192\n",
            "Epoch 96/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7573 - val_loss: 0.8180\n",
            "Epoch 97/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7559 - val_loss: 0.8169\n",
            "Epoch 98/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7546 - val_loss: 0.8158\n",
            "Epoch 99/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7533 - val_loss: 0.8147\n",
            "Epoch 100/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7520 - val_loss: 0.8137\n",
            "Epoch 101/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7507 - val_loss: 0.8127\n",
            "Epoch 102/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7494 - val_loss: 0.8117\n",
            "Epoch 103/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7482 - val_loss: 0.8107\n",
            "Epoch 104/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7470 - val_loss: 0.8097\n",
            "Epoch 105/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7458 - val_loss: 0.8088\n",
            "Epoch 106/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7447 - val_loss: 0.8079\n",
            "Epoch 107/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7435 - val_loss: 0.8070\n",
            "Epoch 108/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7424 - val_loss: 0.8061\n",
            "Epoch 109/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7413 - val_loss: 0.8052\n",
            "Epoch 110/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7402 - val_loss: 0.8044\n",
            "Epoch 111/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7391 - val_loss: 0.8036\n",
            "Epoch 112/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7380 - val_loss: 0.8028\n",
            "Epoch 113/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7370 - val_loss: 0.8020\n",
            "Epoch 114/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7360 - val_loss: 0.8012\n",
            "Epoch 115/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7350 - val_loss: 0.8005\n",
            "Epoch 116/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7340 - val_loss: 0.7997\n",
            "Epoch 117/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7330 - val_loss: 0.7990\n",
            "Epoch 118/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7320 - val_loss: 0.7983\n",
            "Epoch 119/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7311 - val_loss: 0.7976\n",
            "Epoch 120/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7301 - val_loss: 0.7969\n",
            "Epoch 121/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7292 - val_loss: 0.7962\n",
            "Epoch 122/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7283 - val_loss: 0.7956\n",
            "Epoch 123/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7274 - val_loss: 0.7949\n",
            "Epoch 124/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7265 - val_loss: 0.7943\n",
            "Epoch 125/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7257 - val_loss: 0.7937\n",
            "Epoch 126/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7248 - val_loss: 0.7931\n",
            "Epoch 127/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7240 - val_loss: 0.7925\n",
            "Epoch 128/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7231 - val_loss: 0.7919\n",
            "Epoch 129/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7223 - val_loss: 0.7913\n",
            "Epoch 130/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7215 - val_loss: 0.7908\n",
            "Epoch 131/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7207 - val_loss: 0.7902\n",
            "Epoch 132/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7199 - val_loss: 0.7897\n",
            "Epoch 133/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7191 - val_loss: 0.7891\n",
            "Epoch 134/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7183 - val_loss: 0.7886\n",
            "Epoch 135/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7176 - val_loss: 0.7881\n",
            "Epoch 136/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7168 - val_loss: 0.7876\n",
            "Epoch 137/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7161 - val_loss: 0.7871\n",
            "Epoch 138/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7153 - val_loss: 0.7866\n",
            "Epoch 139/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7146 - val_loss: 0.7861\n",
            "Epoch 140/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7139 - val_loss: 0.7857\n",
            "Epoch 141/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7132 - val_loss: 0.7852\n",
            "Epoch 142/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7125 - val_loss: 0.7848\n",
            "Epoch 143/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7118 - val_loss: 0.7843\n",
            "Epoch 144/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7111 - val_loss: 0.7839\n",
            "Epoch 145/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7104 - val_loss: 0.7834\n",
            "Epoch 146/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7098 - val_loss: 0.7830\n",
            "Epoch 147/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7091 - val_loss: 0.7826\n",
            "Epoch 148/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7085 - val_loss: 0.7822\n",
            "Epoch 149/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7078 - val_loss: 0.7818\n",
            "Epoch 150/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7072 - val_loss: 0.7814\n",
            "Epoch 151/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7065 - val_loss: 0.7810\n",
            "Epoch 152/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7059 - val_loss: 0.7806\n",
            "Epoch 153/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7053 - val_loss: 0.7802\n",
            "Epoch 154/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7047 - val_loss: 0.7798\n",
            "Epoch 155/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7041 - val_loss: 0.7795\n",
            "Epoch 156/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7035 - val_loss: 0.7791\n",
            "Epoch 157/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7029 - val_loss: 0.7787\n",
            "Epoch 158/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7023 - val_loss: 0.7784\n",
            "Epoch 159/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7017 - val_loss: 0.7780\n",
            "Epoch 160/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7011 - val_loss: 0.7777\n",
            "Epoch 161/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7006 - val_loss: 0.7773\n",
            "Epoch 162/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7000 - val_loss: 0.7770\n",
            "Epoch 163/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6994 - val_loss: 0.7767\n",
            "Epoch 164/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6989 - val_loss: 0.7763\n",
            "Epoch 165/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6983 - val_loss: 0.7760\n",
            "Epoch 166/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6978 - val_loss: 0.7757\n",
            "Epoch 167/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6973 - val_loss: 0.7754\n",
            "Epoch 168/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6967 - val_loss: 0.7751\n",
            "Epoch 169/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6962 - val_loss: 0.7747\n",
            "Epoch 170/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6957 - val_loss: 0.7744\n",
            "Epoch 171/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6952 - val_loss: 0.7741\n",
            "Epoch 172/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6946 - val_loss: 0.7738\n",
            "Epoch 173/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6941 - val_loss: 0.7735\n",
            "Epoch 174/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6936 - val_loss: 0.7733\n",
            "Epoch 175/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6931 - val_loss: 0.7730\n",
            "Epoch 176/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6926 - val_loss: 0.7727\n",
            "Epoch 177/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6921 - val_loss: 0.7724\n",
            "Epoch 178/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6916 - val_loss: 0.7721\n",
            "Epoch 179/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6912 - val_loss: 0.7718\n",
            "Epoch 180/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6907 - val_loss: 0.7716\n",
            "Epoch 181/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6902 - val_loss: 0.7713\n",
            "Epoch 182/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6897 - val_loss: 0.7710\n",
            "Epoch 183/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6893 - val_loss: 0.7708\n",
            "Epoch 184/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6888 - val_loss: 0.7705\n",
            "Epoch 185/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6883 - val_loss: 0.7702\n",
            "Epoch 186/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6879 - val_loss: 0.7700\n",
            "Epoch 187/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6874 - val_loss: 0.7697\n",
            "Epoch 188/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6870 - val_loss: 0.7695\n",
            "Epoch 189/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6865 - val_loss: 0.7692\n",
            "Epoch 190/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6861 - val_loss: 0.7690\n",
            "Epoch 191/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6857 - val_loss: 0.7687\n",
            "Epoch 192/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6852 - val_loss: 0.7685\n",
            "Epoch 193/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6848 - val_loss: 0.7682\n",
            "Epoch 194/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6844 - val_loss: 0.7680\n",
            "Epoch 195/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6839 - val_loss: 0.7677\n",
            "Epoch 196/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6835 - val_loss: 0.7675\n",
            "Epoch 197/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6831 - val_loss: 0.7673\n",
            "Epoch 198/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6827 - val_loss: 0.7670\n",
            "Epoch 199/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6823 - val_loss: 0.7668\n",
            "Epoch 200/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6818 - val_loss: 0.7666\n",
            "SAE Encoder Weights (W_enc) shape: (20, 100)\n",
            "SAE Encoder Biases (b_enc) shape: (100,)\n",
            "\n",
            "--- Applying Small-World Connectivity to RVFL Hidden Layer ---\n",
            "Sparsity of RVFL effective hidden weights: 15.00%\n",
            "RVFL Effective Hidden Weights (W_rvfl_effective) shape: (20, 100)\n",
            "RVFL Effective Hidden Biases (b_rvfl_effective) shape: (100,)\n",
            "\n",
            "--- Training RVFL Output Layer ---\n",
            "Augmented features (Phi_train) shape: (800, 120)\n",
            "RVFL Output Weights (W_out) shape: (120,)\n",
            "Model training complete.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Test Mean Squared Error (MSE): 115.5312\n",
            "Test R-squared (R2): 0.9977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mcWP77IFRzq4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}